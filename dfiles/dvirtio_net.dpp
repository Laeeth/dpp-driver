import core.stdc.config : c_ulong, c_long;
import std.algorithm.comparison : max, min;
import core.stdc.string : memcpy, memset;

// ****************************************************************************
#include <linux/mod_devicetable.h>
#include <linux/device.h>
#include <linux/workqueue.h>
#include <linux/netdevice.h>
#include <linux/filter.h>
#include <linux/socket.h>
#include <linux/bpf.h>
#include <linux/virtio.h>
#include <linux/virtio_config.h>
#include <linux/virtio_net.h>
#include <linux/cpuhotplug.h>
#include <linux/netdev_features.h>
#include <linux/cpu.h>
// ****************************************************************************

struct control_buf {
    virtio_net_ctrl_hdr hdr;
    ubyte status;
    virtio_net_ctrl_mq mq;
    ubyte promisc;
    ubyte allmulti;
    ushort vid;
    ulong offloads;
}

struct virtnet_sq_stats {
    /* TODO on 32bit */
    //static if (BITS_PER_LONG == 32)
    //{
        //u64_stats_sync syncp;
    //}
    //else
    //{
        //u64_stats_sync[0] syncp;
    //}
    u64_stats_sync[0] syncp;
    ulong packets;
    ulong bytes;
    ulong xdp_tx;
    ulong xdp_tx_drops;
    ulong kicks;
}

struct send_queue {
    virtqueue *vq;
    scatterlist[MAX_SKB_FRAGS + 2] sg;
    char[40] name;
    virtnet_sq_stats stats;
    napi_struct napi;
}

struct ewma_pkt_len {
    c_ulong internal;
}

struct virtnet_rq_stats {
    u64_stats_sync[0] syncp;
    ulong packets;
    ulong bytes;
    ulong drops;
    ulong xdp_packets;
    ulong xdp_tx;
    ulong xdp_redirects;
    ulong xdp_drops;
    ulong kicks;
}

struct receive_queue {
    /* Virtqueue associated with this receive_queue */
    virtqueue *vq;

    napi_struct napi;

    bpf_prog  *xdp_prog;

    virtnet_rq_stats stats;

    /* Chain pages by the private ptr. */
    page *pages;

    /* Average packet length for mergeable receive buffers. */
    ewma_pkt_len mrg_avg_pkt_len;

    /* Page frag for packet buffer allocation. */
    page_frag alloc_frag;

    /* RX: fragments + linear part + virtio header */
    scatterlist[MAX_SKB_FRAGS + 2] sg;

    /* Min single buffer size for mergeable buffers case. */
    uint min_buf_len;

    /* Name of this receive queue: input.$index */
    char[40] name;

    align(SMP_CACHE_BYTES) xdp_rxq_info xdp_rxq;
}

struct dstruct_failover;

struct dlang_virtnet_info {
    send_queue[] sq;
    receive_queue[] rq;
    virtnet_info* vi;
    ubyte[0] tmp;
}

struct virtnet_info {
    virtio_device *vdev;
    virtqueue *cvq;
    net_device *dev;
    send_queue *sq;
    receive_queue *rq;

    uint status;

    /* Max # of queue pairs supported by the device */
    ushort max_queue_pairs;

    /* # of queue pairs currently used by the driver */
    ushort curr_queue_pairs;

    /* # of XDP queue pairs currently used by the driver */
    ushort xdp_queue_pairs;

    /* I like... big packets and I cannot lie! */
    bool big_packets;

    /* Host will merge rx buffers for big packets (shake it! shake it!) */
    bool mergeable_rx_bufs;

    /* Has control virtqueue */
    bool has_cvq;

    /* Host can handle any s/g split between our header and packet data */
    bool any_header_sg;

    /* Packet virtio header size */
    ubyte hdr_len;

    /* Work struct for refilling if we run low on memory. */
    delayed_work refill;

    /* Work struct for config space updates */
    work_struct config_work;

    /* Does the affinity hint is set for virtqueues? */
    bool affinity_hint_set;

    /* CPU hotplug instances for online & dead */
    hlist_node node;
    hlist_node node_dead;

    control_buf *ctrl;

    /* Ethtool settings */
    ubyte duplex;
    uint speed;

    c_ulong guest_offloads;

    /* failover when STANDBY feature enabled */
    dstruct_failover *failover;
}

int bit_macro(int x) {
    return (1 << (x));
}

auto container_of_alex(string type, string member)(void *ptr) {
    void *t = (ptr - mixin(type ~ "." ~ member ~ ".offsetof"));
    return mixin("cast(" ~ type ~ "*) t");
}

auto ARRAY_SIZE_alex(T)(T[] x) {
    return x.length;
}

enum bool csum = true, gso = true;
immutable enum bool napi_tx = false;
immutable enum napi_weight = NAPI_POLL_WEIGHT;

enum GOOD_PACKET_LEN = (ETH_HLEN + VLAN_HLEN + ETH_DATA_LEN);
enum GOOD_COPY_LEN = 128;
enum VIRTNET_RX_PAD = (NET_IP_ALIGN + NET_SKB_PAD);

/* Amount of XDP headroom to prepend to packets for use by xdp_adjust_head */
enum VIRTIO_XDP_HEADROOM = 256;

/* Separating two types of XDP xmit */
enum VIRTIO_XDP_TX = bit_macro(0);
enum VIRTIO_XDP_REDIR = bit_macro(1);

enum VIRTNET_DRIVER_VERSION = "1.0.0";

__gshared immutable enum c_ulong[] guest_offloads = [
    VIRTIO_NET_F_GUEST_TSO4,
    VIRTIO_NET_F_GUEST_TSO6,
    VIRTIO_NET_F_GUEST_ECN,
    VIRTIO_NET_F_GUEST_UFO
];

struct virtnet_stat_desc {
    char[ETH_GSTRING_LEN] desc;
    size_t offset;
}

__gshared immutable enum virtnet_stat_desc[] virtnet_sq_stats_desc = [
    { "packets", virtnet_sq_stats.packets.offsetof },
    { "bytes", virtnet_sq_stats.bytes.offsetof },
    { "xdp_tx", virtnet_sq_stats.xdp_tx.offsetof },
    { "xdp_tx_drops", virtnet_sq_stats.xdp_tx_drops.offsetof },
    { "kicks",  virtnet_sq_stats.kicks.offsetof},
];

__gshared immutable enum virtnet_stat_desc[] virtnet_rq_stats_desc = [
    { "packets", virtnet_rq_stats.packets.offsetof },
    { "bytes", virtnet_rq_stats.bytes.offsetof },
    { "drops", virtnet_rq_stats.drops.offsetof  },
    { "xdp_packets", virtnet_rq_stats.xdp_packets.offsetof  },
    { "xdp_tx", virtnet_rq_stats.xdp_tx.offsetof },
    { "xdp_redirects", virtnet_rq_stats.xdp_redirects.offsetof },
    { "xdp_drops", virtnet_rq_stats.xdp_drops.offsetof },
    { "kicks",  virtnet_sq_stats.kicks.offsetof}
];

enum VIRTNET_SQ_STATS_LEN = virtnet_sq_stats_desc.length;
enum VIRTNET_RQ_STATS_LEN = virtnet_rq_stats_desc.length;

struct padded_vnet_hdr {
    virtio_net_hdr_mrg_rxbuf hdr;
    /*
     * hdr is in a separate sg buffer, and data sg buffer shares same page
     * with this header sg. This padding makes next sg 16 byte aligned
     * after the header.
     */
    char[4] padding;
}

@safe
extern(C) int txq2vq(int txq) {
    return txq * 2 + 1;
}

@safe
extern(C) public int rxq2vq(int rxq)
{
    return rxq * 2;
}

enum int MRG_CTX_HEADER_SHIFT = 22;

@trusted
extern(C) void *mergeable_len_to_ctx(uint truesize, uint headroom)
{
    return cast(void *)(cast(c_ulong)((headroom << MRG_CTX_HEADER_SHIFT) | truesize));
}

@safe
extern(C) uint mergeable_ctx_to_headroom(void *mrg_ctx)
{
    return cast(uint)(cast(c_ulong)(mrg_ctx) >> MRG_CTX_HEADER_SHIFT);
}

@safe
extern(C) uint mergeable_ctx_to_truesize(void *mrg_ctx)
{
    return cast(c_ulong)(mrg_ctx) & ((1 << MRG_CTX_HEADER_SHIFT) - 1);
}

@safe
extern(C) int vq2rxq(const virtqueue *vq)
{
    return vq.index / 2;
}

@safe
extern(C) int vq2txq(const virtqueue *vq)
{
    return (vq.index - 1) / 2;
}

extern(C) void virtqueue_napi_schedule(napi_struct *napi, virtqueue *vq)
{
    if (napi_schedule_prep(napi)) {
        virtqueue_disable_cb(vq);
        __napi_schedule(napi);
    }
}

extern(C) void virtqueue_napi_complete(napi_struct *napi, virtqueue *vq, int processed)
{
    int opaque;

    opaque = virtqueue_enable_cb_prepare(vq);
    if (napi_complete_done(napi, processed)) {
        if (virtqueue_poll(vq, opaque))
            virtqueue_napi_schedule(napi, vq);
    } else {
        virtqueue_disable_cb(vq);
    }
}

@trusted
extern(C) bool virtnet_fail_on_feature(virtio_device *vdev, uint fbit,
        const char *fname, const char *dname)
{
    return virtio_has_feature(vdev, fbit);
}

@trusted
extern(C) virtio_net_hdr_mrg_rxbuf *skb_vnet_hdr(sk_buff *skb)
{
    return cast(virtio_net_hdr_mrg_rxbuf*)skb.cb.ptr;
}

@safe
extern(C) void give_pages(receive_queue *rq, page *pg)
{
    page *end;

    /* Find end of list, sew whole thing into vi->rq.pages. */
    @trusted void helper() {
        for (end = pg; end.private_; end = cast(page *) end.private_)
        {

        }
    }
    end.private_ = cast(c_ulong) rq.pages;
    rq.pages = pg;
}

@trusted
extern(C) page *get_a_page(receive_queue *rq, gfp_t gfp_mask)
{
    page *p = rq.pages;

    if (p !is null) {
        rq.pages = cast(page *) p.private_;
        /* clear private here, it is used to chain pages <] */
        p.private_ = 0;
    } else {
        p = alloc_pages(gfp_mask, 0);
    }

    return p;
}

extern(C) void skb_xmit_done(virtqueue *vq)
{
    virtnet_info *vi;
    napi_struct *napi;

    vi = cast(virtnet_info *) vq.vdev.priv;

    @trusted napi_struct* helper() {
        dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
        assert(dvi.vi == vi);
        return &dvi.sq[vq2txq(vq)].napi;
    }
    napi = helper();
    /* Suppress further interrupts. */
    virtqueue_disable_cb(vq);

    if (napi.weight)
        virtqueue_napi_schedule(napi, vq);
    else
        /* We were probably waiting for more output buffers. */

        //!!!! ciudat, cast la ushort; nu-l facem mai bine?
        netif_wake_subqueue(vi.dev, cast(ushort)vq2txq(vq));
}

extern(C) sk_buff *page_to_skb(virtnet_info *vi, receive_queue *rq,
        page *pg, uint offset, uint len, uint truesize)
{
    sk_buff *skb;
    virtio_net_hdr_mrg_rxbuf *hdr;
    uint copy, hdr_len, hdr_padded_len;
    char *p;

    //posibile probleme
    p = cast(char *)(page_address(pg) + offset);

    /* copy small packet so we can reuse these pages for small data */
    skb = napi_alloc_skb(&rq.napi, GOOD_COPY_LEN);

    if (skb is null)
        return null;

    hdr = skb_vnet_hdr(skb);

    hdr_len = vi.hdr_len;
    if (vi.mergeable_rx_bufs)
        hdr_padded_len = (*hdr).sizeof;
    else
        hdr_padded_len = padded_vnet_hdr.sizeof;

    memcpy(hdr, p, hdr_len);

    len -= hdr_len;
    offset += hdr_padded_len;
    p += hdr_padded_len;

    copy = len;
    if (copy > skb_tailroom(skb))
        copy = skb_tailroom(skb);
    skb_put_data(skb, p, copy);

    len -= copy;
    offset += copy;

    if (vi.mergeable_rx_bufs) {
        if (len)
            skb_add_rx_frag(skb, 0, pg, offset, len, truesize);
        else
            put_page(pg);
        return skb;
    }

    /*
     * Verify that we can indeed put this data into a skb.
     * This is here to handle cases when the device erroneously
     * tries to receive more than is possible. This is usually
     * the case of a broken device.
     */

    if (len > MAX_SKB_FRAGS * PAGE_SIZE) {
        //chestie de debug, cel mai probabil se apeleaza ramura cu no_printk
        //net_dbg_ratelimited("%s: too much data\n", skb.dev.name);
        dev_kfree_skb(skb);
        return null;
    }

    while (len) {
        uint frag_size = min(cast(uint)PAGE_SIZE - offset, len);
        skb_add_rx_frag(skb, (cast(skb_shared_info *) skb_end_pointer(skb)).nr_frags, pg, offset, frag_size, truesize);
        len -= frag_size;
        pg = cast(page *)pg.private_;
        offset = 0;
    }

    if (pg)
        give_pages(rq, pg);

    return skb;
}

extern(C) int __virtnet_xdp_xmit_one(virtnet_info *vi, send_queue *sq, xdp_frame *xdpf)
{
    virtio_net_hdr_mrg_rxbuf *hdr;
    int err;

    /* virtqueue want to use data area in-front of packet */
    if (xdpf.metasize > 0)
        return -EOPNOTSUPP;

    if (xdpf.headroom < vi.hdr_len)
        return -EOVERFLOW;

    /* Make room for virtqueue hdr (also change xdpf->headroom?) */
    xdpf.data -= vi.hdr_len;
    /* Zero header and leave csum up to XDP layers */
    hdr = cast(virtio_net_hdr_mrg_rxbuf *)xdpf.data;
    memset(hdr, 0, vi.hdr_len);
    xdpf.len += vi.hdr_len;

    sg_init_one(sq.sg.ptr, xdpf.data, xdpf.len);

    err = virtqueue_add_outbuf(sq.vq, sq.sg.ptr, 1, xdpf, GFP_ATOMIC);
    if (err)
        return -ENOSPC; /* Caller handle free/refcnt */

    return 0;
}

@trusted
extern(C) uint __dbind__smp_processor_id();

@safe
extern(C) send_queue * virtnet_xdp_sq(virtnet_info *vi)
{
    uint qp;
    qp = vi.curr_queue_pairs - vi.xdp_queue_pairs + __dbind__smp_processor_id();

    @trusted send_queue* helper() {
        dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
        assert(dvi.vi == vi);
        return &dvi.sq[qp];
    }

    return helper();
}

@trusted
extern(C) int virtnet_xdp_xmit(net_device *dev,
        int n,  xdp_frame **frames, uint flags)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    receive_queue *rq = vi.rq;
    xdp_frame *xdpf_sent;
    bpf_prog *xdp_prog;
    send_queue *sq;
    uint len;
    int drops = 0;
    int kicks = 0;
    int ret, err;
    int i;

    sq = virtnet_xdp_sq(vi);

    if (flags & ~XDP_XMIT_FLAGS_MASK) {
        ret = -EINVAL;
        drops = n;
        goto out_label;
    }

     //Only allow ndo_xdp_xmit if XDP is loaded on dev, as this
     //indicate XDP resources have been successfully allocated.
    xdp_prog = rq.xdp_prog;
    if (xdp_prog is null) {
        ret = -ENXIO;
        drops = n;
        goto out_label;
    }

    // Free up any pending old buffers before queueing new ones.
    while ((xdpf_sent = cast(xdp_frame *)virtqueue_get_buf(sq.vq, &len)) !is null)
        xdp_return_frame(xdpf_sent);

    for (i = 0; i < n; i++) {
        xdp_frame *xdpf = frames[i];

        err = __virtnet_xdp_xmit_one(vi, sq, xdpf);
        if (err) {
            xdp_return_frame_rx_napi(xdpf);
            drops++;
        }
    }
    ret = n - drops;

    if (flags & XDP_XMIT_FLUSH) {
        if (virtqueue_kick_prepare(sq.vq) && virtqueue_notify(sq.vq))
            kicks = 1;
    }

out_label:
    u64_stats_update_begin(sq.stats.syncp.ptr);
    sq.stats.xdp_tx += n;
    sq.stats.xdp_tx_drops += drops;
    sq.stats.kicks += kicks;
    u64_stats_update_end(sq.stats.syncp.ptr);

    return ret;
}

extern(C) uint virtnet_get_headroom(virtnet_info *vi)
{
    return vi.xdp_queue_pairs ? VIRTIO_XDP_HEADROOM : 0;
}

extern(C) page *xdp_linearize_page(receive_queue *rq,
                       ushort *num_buf,
                       page *p,
                       int offset,
                       int page_off,
                       uint *len)
{
    page *pg = alloc_pages(GFP_ATOMIC, 0);

    if (pg is null)
        return null;

    memcpy(page_address(pg) + page_off, page_address(p) + offset, *len);
    page_off += *len;

    while (--*num_buf) {
        int tailroom = cast(int) SKB_DATA_ALIGN(skb_shared_info.sizeof);
        uint buflen;
        void *buf;
        int off;

        buf = virtqueue_get_buf(rq.vq, &buflen);
        if (buf is null)
            goto err_buf;

        p = virt_to_head_page(buf);
        off = cast(int)(buf - page_address(p));

        /* guard against a misconfigured or uncooperative backend that
         * is sending packet larger than the MTU.
         */
        if ((page_off + buflen + tailroom) > PAGE_SIZE) {
            put_page(p);
            goto err_buf;
        }

        memcpy(page_address(pg) + page_off,
               page_address(p) + off, buflen);
        page_off += buflen;
        put_page(p);
    }

    /* Headroom does not contribute to packet length */
    *len = page_off - VIRTIO_XDP_HEADROOM;
    return pg;
err_buf:
    __free_pages(pg, 0);
    return null;
}

extern(C)  sk_buff *receive_small(net_device *dev,
                    virtnet_info *vi,
                    receive_queue *rq,
                    void *buf, void *ctx,
                    uint len,
                    uint *xdp_xmit,
                    virtnet_rq_stats *stats)
{
    sk_buff *skb;
    bpf_prog *xdp_prog;
    uint xdp_headroom = cast(uint)(cast(c_ulong)ctx);
    uint header_offset = VIRTNET_RX_PAD + xdp_headroom;
    uint headroom = vi.hdr_len + header_offset;
    uint buflen = cast(uint) (SKB_DATA_ALIGN(GOOD_PACKET_LEN + headroom) +
                              SKB_DATA_ALIGN(skb_shared_info.sizeof));
    page *pg = virt_to_head_page(buf);
    uint delta = 0;
    page *xdp_page;
    int err;

    len -= vi.hdr_len;
    stats.bytes += len;

    rcu_read_lock();
    xdp_prog = rq.xdp_prog;
    if (xdp_prog) {
        virtio_net_hdr_mrg_rxbuf *hdr = cast(virtio_net_hdr_mrg_rxbuf *)(buf + header_offset);
        xdp_frame *xdpf;
        xdp_buff xdp;
        void *orig_data;
        uint act;

        if (hdr.hdr.gso_type)
            goto err_xdp;

        if (xdp_headroom < virtnet_get_headroom(vi)) {
            int offset = cast(int)(buf - page_address(pg) + header_offset);
            uint tlen = len + vi.hdr_len;
            ushort num_buf = 1;

            xdp_headroom = virtnet_get_headroom(vi);
            header_offset = VIRTNET_RX_PAD + xdp_headroom;
            headroom = vi.hdr_len + header_offset;
            buflen = cast(uint) (SKB_DATA_ALIGN(GOOD_PACKET_LEN + headroom) +
                                 SKB_DATA_ALIGN(skb_shared_info.sizeof));
            xdp_page = xdp_linearize_page(rq, &num_buf, pg,
                              offset, header_offset,
                              &tlen);
            if (xdp_page is null)
                goto err_xdp;

            buf = page_address(xdp_page);
            put_page(pg);
            pg = xdp_page;
        }

        xdp.data_hard_start = buf + VIRTNET_RX_PAD + vi.hdr_len;
        xdp.data = xdp.data_hard_start + xdp_headroom;
        xdp_set_data_meta_invalid(&xdp);
        xdp.data_end = xdp.data + len;
        xdp.rxq = &rq.xdp_rxq;
        orig_data = xdp.data;
        act = bpf_prog_run_xdp(xdp_prog, &xdp);
        stats.xdp_packets++;

        switch (act) {
        case xdp_action.XDP_PASS:
            //[> Recalculate length in case bpf program changed it <]
            delta = cast(uint)(orig_data - xdp.data);
            len = cast(uint)(xdp.data_end - xdp.data);
            break;
        case xdp_action.XDP_TX:
            stats.xdp_tx++;
            xdpf = convert_to_xdp_frame(&xdp);

            if (xdpf is null)
                goto err_xdp;
            err = virtnet_xdp_xmit(dev, 1, &xdpf, 0);
            if (err < 0) {
                goto err_xdp;
            }
            *xdp_xmit |= VIRTIO_XDP_TX;
            rcu_read_unlock();
            goto xdp_xmit;
        case xdp_action.XDP_REDIRECT:
            stats.xdp_redirects++;
            err = xdp_do_redirect(dev, &xdp, xdp_prog);
            if (err)
                goto err_xdp;
            *xdp_xmit |= VIRTIO_XDP_REDIR;
            rcu_read_unlock();
            goto xdp_xmit;
        default:
            bpf_warn_invalid_xdp_action(act);
            goto case;
            //[> fall through <]
        case xdp_action.XDP_ABORTED:
            goto case;
        case xdp_action.XDP_DROP:
            goto err_xdp;
        }
    }
    rcu_read_unlock();

    skb = build_skb(buf, buflen);
    if (skb is null) {
        put_page(pg);
        goto err;
    }
    skb_reserve(skb, headroom - delta);
    skb_put(skb, len);
    if (!delta) {
        buf += header_offset;
        memcpy(skb_vnet_hdr(skb), buf, vi.hdr_len);
    // [> keep zeroed vnet hdr since packet was changed by bpf <]
    }

err:
    return skb;

err_xdp:
    rcu_read_unlock();
    stats.xdp_drops++;
    stats.drops++;
    put_page(pg);
xdp_xmit:
    return null;
}

extern(C) sk_buff *receive_big(net_device *dev,
                   virtnet_info *vi,
                   receive_queue *rq,
                   void *buf,
                   uint len,
                   virtnet_rq_stats *stats)
{
    page *pg = cast(page *)buf;
    sk_buff *skb = page_to_skb(vi, rq, pg, 0, len, PAGE_SIZE);

    stats.bytes += len - vi.hdr_len;
    if (skb is null)
        goto err;

    return skb;

err:
    stats.drops++;
    give_pages(rq, pg);
    return null;
}

extern(C) void __dbind__ewma_pkt_len_add(ewma_pkt_len *e, c_ulong val);
extern(C) c_ulong __dbind__ewma_pkt_len_read(ewma_pkt_len *e);

extern(C) sk_buff *receive_mergeable(net_device *dev,
                    virtnet_info *vi,
                    receive_queue *rq,
                    void *buf,
                    void *ctx,
                    uint len,
                    uint *xdp_xmit,
                    virtnet_rq_stats *stats)
{
    virtio_net_hdr_mrg_rxbuf *hdr = cast(virtio_net_hdr_mrg_rxbuf *)buf;
    ushort num_buf = virtio16_to_cpu(vi.vdev, hdr.num_buffers);
    page *pg = virt_to_head_page(buf);
    int offset = cast(int)(buf - page_address(pg));
    sk_buff *head_skb;
    sk_buff *curr_skb;
    bpf_prog *xdp_prog;
    uint truesize;
    uint headroom = mergeable_ctx_to_headroom(ctx);
    int err;

    head_skb = null;
    stats.bytes += len - vi.hdr_len;

    rcu_read_lock();
    xdp_prog = rq.xdp_prog;
    if (xdp_prog) {
        xdp_frame *xdpf;
        page *xdp_page;
        xdp_buff xdp;
        void *data;
        uint act;

        /* Transient failure which in theory could occur if
         * in-flight packets from before XDP was enabled reach
         * the receive path after XDP is loaded.
         */
        if (hdr.hdr.gso_type)
            goto err_xdp;

        /* This happens when rx buffer size is underestimated
         * or headroom is not enough because of the buffer
         * was refilled before XDP is set. This should only
         * happen for the first several packets, so we don't
         * care much about its performance.
         */

        if (num_buf > 1 || headroom < virtnet_get_headroom(vi)) {
            /* linearize data for XDP */
            xdp_page = xdp_linearize_page(rq, &num_buf,
                              pg, offset,
                              VIRTIO_XDP_HEADROOM,
                              &len);
            if (xdp_page is null)
                goto err_xdp;
            offset = VIRTIO_XDP_HEADROOM;
        } else {
            xdp_page = pg;
        }

        /* Allow consuming headroom but reserve enough space to push
         * the descriptor on if we get an XDP_TX return code.
         */
        data = page_address(xdp_page) + offset;
        xdp.data_hard_start = data - VIRTIO_XDP_HEADROOM + vi.hdr_len;
        xdp.data = data + vi.hdr_len;
        xdp_set_data_meta_invalid(&xdp);
        xdp.data_end = xdp.data + (len - vi.hdr_len);
        xdp.rxq = &rq.xdp_rxq;

        act = bpf_prog_run_xdp(xdp_prog, &xdp);
        stats.xdp_packets++;

        switch (act) {
        case xdp_action.XDP_PASS:
            /* recalculate offset to account for any header
             * adjustments. Note other cases do not build an
             * skb and avoid using offset
             */
            offset = cast(int)(xdp.data -
                    page_address(xdp_page) - vi.hdr_len);

            /* recalculate len if xdp.data or xdp.data_end were
             * adjusted
             */
            len = cast(uint)(xdp.data_end - xdp.data + vi.hdr_len);
            /* We can only create skb based on xdp_page. */
            if (xdp_page != pg) {
                rcu_read_unlock();
                put_page(pg);
                head_skb = page_to_skb(vi, rq, xdp_page,
                               offset, len, PAGE_SIZE);
                return head_skb;
            }
            break;
        case xdp_action.XDP_TX:
            stats.xdp_tx++;
            xdpf = convert_to_xdp_frame(&xdp);
            if (xdpf is null)
                goto err_xdp;
            err = virtnet_xdp_xmit(dev, 1, &xdpf, 0);
            if (err < 0) {
                if (xdp_page != pg)
                    put_page(xdp_page);
                goto err_xdp;
            }
            *xdp_xmit |= VIRTIO_XDP_TX;
            if (xdp_page != pg)
                put_page(pg);
            rcu_read_unlock();
            goto xdp_xmit;
        case xdp_action.XDP_REDIRECT:
            stats.xdp_redirects++;
            err = xdp_do_redirect(dev, &xdp, xdp_prog);
            if (err) {
                if (xdp_page != pg)
                    put_page(xdp_page);
                goto err_xdp;
            }
            *xdp_xmit |= VIRTIO_XDP_REDIR;
            if (xdp_page != pg)
                put_page(pg);
            rcu_read_unlock();
            goto xdp_xmit;
        default:
            bpf_warn_invalid_xdp_action(act);
            goto case;
            /* fall through */
        case xdp_action.XDP_ABORTED:
            goto case;
            /* fall through */
        case xdp_action.XDP_DROP:
            if (xdp_page != pg)
                __free_pages(xdp_page, 0);
            goto err_xdp;
        }
    }
    rcu_read_unlock();

    truesize = mergeable_ctx_to_truesize(ctx);
    if (len > truesize) {
        dev.stats.rx_length_errors++;
        goto err_skb;
    }

    head_skb = page_to_skb(vi, rq, pg, offset, len, truesize);
    curr_skb = head_skb;

    if (curr_skb is null)
        goto err_skb;
    while (--num_buf) {
        int num_skb_frags;

        buf = virtqueue_get_buf_ctx(rq.vq, &len, &ctx);
        if (buf is null) {
            dev.stats.rx_length_errors++;
            goto err_buf;
        }

        stats.bytes += len;
        pg = virt_to_head_page(buf);

        truesize = mergeable_ctx_to_truesize(ctx);
        if (len > truesize) {
            dev.stats.rx_length_errors++;
            goto err_skb;
        }

        num_skb_frags = (cast(skb_shared_info *) skb_end_pointer(curr_skb)).nr_frags;
        if (num_skb_frags == MAX_SKB_FRAGS) {
             sk_buff *nskb = alloc_skb(0, GFP_ATOMIC);

            if (nskb is null)
                goto err_skb;
            if (curr_skb == head_skb)
                (cast(skb_shared_info *) skb_end_pointer(curr_skb)).frag_list = nskb;
            else
                curr_skb.next = nskb;
            curr_skb = nskb;
            head_skb.truesize += nskb.truesize;
            num_skb_frags = 0;
        }
        if (curr_skb != head_skb) {
            head_skb.data_len += len;
            head_skb.len += len;
            head_skb.truesize += truesize;
        }
        offset = cast(int)(buf - page_address(pg));
        if (skb_can_coalesce(curr_skb, num_skb_frags, pg, offset)) {
            put_page(pg);
            skb_coalesce_rx_frag(curr_skb, num_skb_frags - 1,
                         len, truesize);
        } else {
            skb_add_rx_frag(curr_skb, num_skb_frags, pg,
                    offset, len, truesize);
        }
    }

    __dbind__ewma_pkt_len_add(&rq.mrg_avg_pkt_len, head_skb.len);
    return head_skb;

err_xdp:
    rcu_read_unlock();
    stats.xdp_drops++;
err_skb:
    put_page(pg);
    while (num_buf-- > 1) {
        buf = virtqueue_get_buf(rq.vq, &len);
        if (buf is null) {
            dev.stats.rx_length_errors++;
            break;
        }
        stats.bytes += len;
        pg = virt_to_head_page(buf);
        put_page(pg);
    }
err_buf:
    stats.drops++;
    dev_kfree_skb(head_skb);
xdp_xmit:
    return null;
}

extern(C) void receive_buf(virtnet_info *vi, receive_queue *rq,
            void *buf, uint len, void **ctx,
            uint *xdp_xmit,
            virtnet_rq_stats *stats)
{
    net_device *dev = vi.dev;
    sk_buff *skb;
    virtio_net_hdr_mrg_rxbuf *hdr;

    if (len < vi.hdr_len + ETH_HLEN) {
        dev.stats.rx_length_errors++;
        if (vi.mergeable_rx_bufs) {
            put_page(virt_to_head_page(buf));
        } else if (vi.big_packets) {
            give_pages(rq, cast(page *)buf);
        } else {
            put_page(virt_to_head_page(buf));
        }
        return;
    }

    if (vi.mergeable_rx_bufs)
        skb = receive_mergeable(dev, vi, rq, buf, ctx, len, xdp_xmit,
                    stats);
    else if (vi.big_packets)
        skb = receive_big(dev, vi, rq, buf, len, stats);
    else
        skb = receive_small(dev, vi, rq, buf, ctx, len, xdp_xmit, stats);

    if (skb is null)
        return;

    hdr = skb_vnet_hdr(skb);

    if (hdr.hdr.flags & VIRTIO_NET_HDR_F_DATA_VALID)
        skb.ip_summed = CHECKSUM_UNNECESSARY;
        //skb.cloned= 0;
        //dev.wol_enabled= 0;

    if (virtio_net_hdr_to_skb(skb, &hdr.hdr,
                  virtio_is_little_endian(vi.vdev))) {
        //net_warn_ratelimited("%s: bad gso: type: %u, size: %u\n",
                     //dev.name, hdr.hdr.gso_type,
                     //hdr.hdr.gso_size);
        goto frame_err;
    }

    skb.protocol = eth_type_trans(skb, dev);
    napi_gro_receive(&rq.napi, skb);
    return;

frame_err:
    dev.stats.rx_frame_errors++;
    dev_kfree_skb(skb);
}

extern(C) int add_recvbuf_small(virtnet_info *vi,receive_queue *rq,
             gfp_t gfp)
{
    page_frag *alloc_frag = &rq.alloc_frag;
    char *buf;
    uint xdp_headroom = virtnet_get_headroom(vi);
    void *ctx = cast(void *)(cast(c_ulong)(xdp_headroom));
    int len = vi.hdr_len + VIRTNET_RX_PAD + GOOD_PACKET_LEN + xdp_headroom;
    int err;

    len = cast(uint) (SKB_DATA_ALIGN(len) + SKB_DATA_ALIGN(skb_shared_info.sizeof));
    if (!skb_page_frag_refill(len, alloc_frag, gfp))
        return -ENOMEM;

    buf = cast(char *)page_address(alloc_frag.page_) + alloc_frag.offset;
    get_page(alloc_frag.page_);
    alloc_frag.offset += len;
    sg_init_one(rq.sg.ptr, buf + VIRTNET_RX_PAD + xdp_headroom,
            vi.hdr_len + GOOD_PACKET_LEN);
    err = virtqueue_add_inbuf_ctx(rq.vq, rq.sg.ptr, 1, buf, ctx, gfp);
    if (err < 0)
        put_page(virt_to_head_page(buf));
    return err;
}

extern(C) int add_recvbuf_big(virtnet_info *vi, receive_queue *rq,
               gfp_t gfp)
{
    page *first = null;
    page *list = null;
    char *p;
    int i, err, offset;

    sg_init_table(rq.sg.ptr, MAX_SKB_FRAGS + 2);

    for (i = MAX_SKB_FRAGS + 1; i > 1; --i) {
        first = get_a_page(rq, gfp);
        if (first is null) {
            if (list !is null)
                give_pages(rq, list);
            return -ENOMEM;
        }
        sg_set_buf(&rq.sg[i], page_address(first), PAGE_SIZE);

        first.private_ = cast(c_ulong)list;
        list = first;
    }

    first = get_a_page(rq, gfp);
    if (first is null) {
        give_pages(rq, list);
        return -ENOMEM;
    }
    p = cast(char *)page_address(first);

    sg_set_buf(&rq.sg[0], p, vi.hdr_len);

    offset = padded_vnet_hdr.sizeof;
    sg_set_buf(&rq.sg[1], p + offset, cast(uint) PAGE_SIZE - offset);

    first.private_ = cast(c_ulong)list;
    err = virtqueue_add_inbuf(rq.vq, rq.sg.ptr, MAX_SKB_FRAGS + 2,
                  first, gfp);
    if (err < 0)
        give_pages(rq, first);

    return err;
}

extern(C) uint __dbind__clamp_t(c_ulong, uint, size_t);

extern(C) uint get_mergeable_buf_len(receive_queue *rq,
                      ewma_pkt_len *avg_pkt_len,
                      uint room)
{
    const size_t hdr_len = virtio_net_hdr_mrg_rxbuf.sizeof;
    uint len;

    if (room)
        return cast(uint) PAGE_SIZE - room;

    len = cast(uint)(hdr_len + __dbind__clamp_t(__dbind__ewma_pkt_len_read(avg_pkt_len),
                rq.min_buf_len, PAGE_SIZE - hdr_len));

    return cast(uint)ALIGN(len, L1_CACHE_BYTES);
}

extern(C) int add_recvbuf_mergeable(virtnet_info *vi,
                 receive_queue *rq, gfp_t gfp)
{
    page_frag *alloc_frag = &rq.alloc_frag;
    uint headroom = virtnet_get_headroom(vi);
    uint tailroom = headroom ? (skb_shared_info.sizeof) : 0;
    uint room = cast(uint) SKB_DATA_ALIGN(headroom + tailroom);
    char *buf;
    void *ctx;
    int err;
    uint len, hole;

    /* Extra tailroom is needed to satisfy XDP's assumption. This
     * means rx frags coalescing won't work, but consider we've
     * disabled GSO for XDP, it won't be a big issue.
     */
    len = get_mergeable_buf_len(rq, &rq.mrg_avg_pkt_len, room);
    if (!skb_page_frag_refill(len + room, alloc_frag, gfp))
        return -ENOMEM;

    buf = cast(char *)page_address(alloc_frag.page_) + alloc_frag.offset;
    buf += headroom; /* advance address leaving hole at front of pkt */
    get_page(alloc_frag.page_);
    alloc_frag.offset += len + room;
    hole = alloc_frag.size - alloc_frag.offset;
    if (hole < len + room) {
        /* To avoid internal fragmentation, if there is very likely not
         * enough space for another buffer, add the remaining space to
         * the current buffer.
         */
        len += hole;
        alloc_frag.offset += hole;
    }

    sg_init_one(rq.sg.ptr, buf, len);
    ctx = mergeable_len_to_ctx(len, headroom);
    err = virtqueue_add_inbuf_ctx(rq.vq, rq.sg.ptr, 1, buf, ctx, gfp);
    if (err < 0)
        put_page(virt_to_head_page(buf));

    return err;
}

extern(C) bool try_fill_recv(virtnet_info *vi, receive_queue *rq, gfp_t gfp)
{
    int err;
    bool oom;

    do {
        if (vi.mergeable_rx_bufs)
            err = add_recvbuf_mergeable(vi, rq, gfp);
        else if (vi.big_packets)
            err = add_recvbuf_big(vi, rq, gfp);
        else
            err = add_recvbuf_small(vi, rq, gfp);

        oom = err == -ENOMEM;
        if (err)
            break;
    } while (rq.vq.num_free);
    if (virtqueue_kick_prepare(rq.vq) && virtqueue_notify(rq.vq)) {
        u64_stats_update_begin(rq.stats.syncp.ptr);
        rq.stats.kicks++;
        u64_stats_update_end(rq.stats.syncp.ptr);
    }

    return !oom;
}

extern(C) void skb_recv_done(virtqueue *rvq)
{
    virtnet_info *vi = cast(virtnet_info *) rvq.vdev.priv;
    receive_queue *rq;

    @trusted receive_queue* helper() {
        dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
        assert(dvi.vi == vi);
        return &dvi.rq[vq2rxq(rvq)];
    }
    rq = helper();

    virtqueue_napi_schedule(&rq.napi, rvq);
}

extern(C) void virtnet_napi_enable(virtqueue *vq, napi_struct *napi)
{
    napi_enable(napi);

    local_bh_disable();
    virtqueue_napi_schedule(napi, vq);
    local_bh_enable();
}

extern(C) void virtnet_napi_tx_enable(virtnet_info *vi,
        virtqueue *vq, napi_struct *napi)
{
    if (!napi.weight)
        return;

    if (!vi.affinity_hint_set) {
        napi.weight = 0;
        return;
    }

    return virtnet_napi_enable(vq, napi);
}

extern(C) void virtnet_napi_tx_disable(napi_struct *napi)
{
    if (napi.weight)
        napi_disable(napi);
}

extern(C) void refill_work(work_struct *work)
{
    //era refill.work, dar work are offset 0 in refill
    virtnet_info *vi = container_of_alex!("virtnet_info", "refill")(work);
    bool still_empty;
    int i;

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);
    assert(vi.curr_queue_pairs <= dvi.rq.length);

    for (i = 0; i < vi.curr_queue_pairs; i++) {
        receive_queue *rq = &vi.rq[i];

        napi_disable(&rq.napi);
        still_empty = !try_fill_recv(vi, rq, GFP_KERNEL);
        virtnet_napi_enable(rq.vq, &rq.napi);

        if (still_empty)
            schedule_delayed_work(&vi.refill, HZ/2);
    }
}

extern(C) int virtnet_receive(receive_queue *rq, int budget,
               uint *xdp_xmit)
{
    virtnet_info *vi = cast(virtnet_info *) rq.vq.vdev.priv;
    virtnet_rq_stats stats;
    uint len;
    void *buf;
    int i;

    if (!vi.big_packets || vi.mergeable_rx_bufs) {
        void *ctx;

        while ((stats.packets < budget) && ((buf = virtqueue_get_buf_ctx(rq.vq, &len, &ctx)) !is null))   {
            receive_buf(vi, rq, buf, len, cast(void**) ctx, xdp_xmit, &stats);
            stats.packets++;
        }
    } else {
        while (stats.packets < budget && ((buf = virtqueue_get_buf(rq.vq, &len)) !is null)) {
            receive_buf(vi, rq, buf, len, null, xdp_xmit, &stats);
            stats.packets++;
        }
    }

    if (rq.vq.num_free > virtqueue_get_vring_size(rq.vq) / 2) {
        if (!try_fill_recv(vi, rq, GFP_ATOMIC))
            schedule_delayed_work(&vi.refill, 0);
    }

    u64_stats_update_begin(rq.stats.syncp.ptr);
    for (i = 0; i < VIRTNET_RQ_STATS_LEN; i++) {
        size_t offset = virtnet_rq_stats_desc[i].offset;
        ulong *item;

        item = cast(ulong *)(cast(ubyte *)&rq.stats + offset);
        *item += *(cast(ulong *)(cast(ubyte *)&stats + offset));
    }
    u64_stats_update_end(rq.stats.syncp.ptr);

    return cast(int) stats.packets;
}

extern(C) void free_old_xmit_skbs( send_queue *sq)
{
    sk_buff *skb;
    uint len;
    uint packets = 0;
    uint bytes = 0;

    while ((skb = cast(sk_buff *)(virtqueue_get_buf(sq.vq, &len))) !is null) {
        bytes += skb.len;
        packets++;

        dev_consume_skb_any(skb);
    }

    /* Avoid overhead when no packets have been processed
     * happens when called speculatively from start_xmit.
     */
    if (!packets)
        return;

    u64_stats_update_begin(sq.stats.syncp.ptr);
    sq.stats.bytes += bytes;
    sq.stats.packets += packets;
    u64_stats_update_end(sq.stats.syncp.ptr);
}

extern(C) void virtnet_poll_cleantx( receive_queue *rq)
{
    virtnet_info *vi = cast(virtnet_info *) rq.vq.vdev.priv;
    uint index = vq2rxq(rq.vq);
    send_queue *sq;

    @trusted send_queue* helper() {
        dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
        assert(dvi.vi == vi);
        return &dvi.sq[index];
    }
    sq = helper();

    netdev_queue *txq = netdev_get_tx_queue(vi.dev, index);

    if (!sq.napi.weight)
        return;

    if (__netif_tx_trylock(txq)) {
        free_old_xmit_skbs(sq);
        __netif_tx_unlock(txq);
    }

    if (sq.vq.num_free >= 2 + MAX_SKB_FRAGS)
        netif_tx_wake_queue(txq);
}

extern(C) int virtnet_poll(napi_struct *napi, int budget)
{
    receive_queue *rq = container_of_alex!("receive_queue", "napi")(napi);
    virtnet_info *vi = cast(virtnet_info *) rq.vq.vdev.priv;
    send_queue *sq;
    uint received;
    uint xdp_xmit = 0;

    virtnet_poll_cleantx(rq);

    received = virtnet_receive(rq, budget, &xdp_xmit);

    /* Out of packets? */
    if (received < budget)
        virtqueue_napi_complete(napi, rq.vq, received);

    if (xdp_xmit & VIRTIO_XDP_REDIR)
        xdp_do_flush_map();

    if (xdp_xmit & VIRTIO_XDP_TX) {
        sq = virtnet_xdp_sq(vi);
        if (virtqueue_kick_prepare(sq.vq) && virtqueue_notify(sq.vq)) {
            u64_stats_update_begin(sq.stats.syncp.ptr);
            sq.stats.kicks++;
            u64_stats_update_end(sq.stats.syncp.ptr);
        }
    }

    return received;
}

extern(C) int virtnet_open(net_device *dev)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    int i, err;

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);

    assert(dvi.vi == vi);
    assert(vi.max_queue_pairs <= dvi.rq.length);
    assert(vi.max_queue_pairs <= dvi.sq.length);

    for (i = 0; i < vi.max_queue_pairs; i++) {
        if (i < vi.curr_queue_pairs)
            /* Make sure we have some buffers: if oom use wq. */
            if (!try_fill_recv(vi, &vi.rq[i], GFP_KERNEL))
                schedule_delayed_work(&vi.refill, 0);

        err = xdp_rxq_info_reg(&vi.rq[i].xdp_rxq, dev, i);
        if (err < 0)
            return err;

        err = xdp_rxq_info_reg_mem_model(&vi.rq[i].xdp_rxq,
                         xdp_mem_type.MEM_TYPE_PAGE_SHARED, null);
        if (err < 0) {
            xdp_rxq_info_unreg(&vi.rq[i].xdp_rxq);
            return err;
        }

        virtnet_napi_enable(vi.rq[i].vq, &vi.rq[i].napi);
        virtnet_napi_tx_enable(vi, vi.sq[i].vq, &vi.sq[i].napi);
    }

    return 0;
}

extern(C) int __dbind__raw_smp_processor_id();

extern(C) int virtnet_poll_tx(napi_struct *napi, int budget)
{
     send_queue *sq = container_of_alex!("send_queue", "napi")(napi);
     virtnet_info *vi = cast(virtnet_info *) sq.vq.vdev.priv;
     netdev_queue *txq = netdev_get_tx_queue(vi.dev, vq2txq(sq.vq));

    __netif_tx_lock(txq, __dbind__raw_smp_processor_id());
    free_old_xmit_skbs(sq);
    __netif_tx_unlock(txq);

    virtqueue_napi_complete(napi, sq.vq, 0);

    if (sq.vq.num_free >= 2 + MAX_SKB_FRAGS)
        netif_tx_wake_queue(txq);

    return 0;
}

extern(C) void __dbind__print_bug();

extern(C) int xmit_skb(send_queue *sq,  sk_buff *skb)
{
    virtio_net_hdr_mrg_rxbuf *hdr;
    const ubyte * dest = cast(ubyte *)((cast (ethhdr *)skb.data).h_dest);
    virtnet_info *vi = cast(virtnet_info *) sq.vq.vdev.priv;
    int num_sg;
    uint hdr_len = vi.hdr_len;
    bool can_push;

    can_push = vi.any_header_sg &&
        //grija la nebunia asta !!!!!!__alignof__(*hdr)
        !(cast(c_ulong)skb.data & (virtio_net_hdr_mrg_rxbuf.alignof - 1)) &&
        !skb_header_cloned(skb) && skb_headroom(skb) >= hdr_len;
    /* Even if we can, don't push here yet as this would skew
     * csum_start offset below. */
    if (can_push)
        hdr = cast(virtio_net_hdr_mrg_rxbuf *)(skb.data - hdr_len);
    else
        hdr = skb_vnet_hdr(skb);

    if (virtio_net_hdr_from_skb(skb, &hdr.hdr,
                    virtio_is_little_endian(vi.vdev), false,
                    0))
        __dbind__print_bug();

    if (vi.mergeable_rx_bufs)
        hdr.num_buffers = 0;

    sg_init_table(sq.sg.ptr, (cast(skb_shared_info *) skb_end_pointer(skb)).nr_frags + (can_push ? 1 : 2));
    if (can_push) {
        skb_push(skb, hdr_len);
        num_sg = skb_to_sgvec(skb, sq.sg.ptr, 0, skb.len);
        if (num_sg < 0)
            return num_sg;
        /* Pull header back to avoid skew in tx bytes calculations. */
        skb_pull(skb, hdr_len);
    } else {
        sg_set_buf(sq.sg.ptr, hdr, hdr_len);
        //grija aici!!!!!!!!
        num_sg = skb_to_sgvec(skb, sq.sg.ptr + 1, 0, skb.len);
        if (num_sg < 0)
            return num_sg;
        num_sg++;
    }
    return virtqueue_add_outbuf(sq.vq, sq.sg.ptr, num_sg, skb, GFP_ATOMIC);
}

extern(C) ubyte __dbind__get_xmit_more_bitfield(sk_buff *);

extern(C) netdev_tx_t start_xmit(sk_buff *skb, net_device *dev)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    int qnum = skb_get_queue_mapping(skb);
    send_queue *sq;

    @trusted send_queue* helper() {
        dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
        assert(dvi.vi == vi);
        return &dvi.sq[qnum];
    }
    sq = helper();

    int err;
    netdev_queue *txq = netdev_get_tx_queue(dev, qnum);
    bool kick = !skb.xmit_more;
    bool use_napi = cast(bool)sq.napi.weight;

    /* Free up any pending old buffers before queueing new ones. */
    free_old_xmit_skbs(sq);

    if (use_napi && kick)
        virtqueue_enable_cb_delayed(sq.vq);

    /* timestamp packet in software */
    skb_tx_timestamp(skb);

    /* Try to transmit */
    err = xmit_skb(sq, skb);

    /* This should not happen! */
    if (err) {
        dev.stats.tx_fifo_errors++;
        if (net_ratelimit())
            __dbind__print_bug();
        dev.stats.tx_dropped++;
        dev_kfree_skb_any(skb);
        return netdev_tx.NETDEV_TX_OK;
    }

    /* Don't wait up for transmitted skbs to be freed. */
    if (!use_napi) {
        skb_orphan(skb);
        nf_reset(skb);
    }

    /* If running out of space, stop queue to avoid getting packets that we
     * are then unable to transmit.
     * An alternative would be to force queuing layer to requeue the skb by
     * returning NETDEV_TX_BUSY. However, NETDEV_TX_BUSY should not be
     * returned in a normal path of operation: it means that driver is not
     * maintaining the TX queue stop/start state properly, and causes
     * the stack to do a non-trivial amount of useless work.
     * Since most packets only take 1 or 2 ring slots, stopping the queue
     * early means 16 slots are typically wasted.
     */
    if (sq.vq.num_free < 2+MAX_SKB_FRAGS) {
        netif_stop_subqueue(dev, cast(ushort)qnum);
        if (!use_napi &&
            !virtqueue_enable_cb_delayed(sq.vq)) {
            /* More just got used, free them then recheck. */
            free_old_xmit_skbs(sq);
            if (sq.vq.num_free >= 2+MAX_SKB_FRAGS) {
                netif_start_subqueue(dev, cast(ushort)qnum);
                virtqueue_disable_cb(sq.vq);
            }
        }
    }

    if (kick || netif_xmit_stopped(txq)) {
        if (virtqueue_kick_prepare(sq.vq) && virtqueue_notify(sq.vq)) {
            u64_stats_update_begin(sq.stats.syncp.ptr);
            sq.stats.kicks++;
            u64_stats_update_end(sq.stats.syncp.ptr);
        }
    }

    return netdev_tx.NETDEV_TX_OK;
}

extern(C) bool virtnet_send_command(virtnet_info *vi, ubyte dlang_class_alias, ubyte cmd,
                  scatterlist *dlang_out_alias)
{
    scatterlist*[4] sgs;
    scatterlist hdr, stat;
    uint out_num = 0;
    uint tmp;

    assert(virtio_has_feature(vi.vdev, VIRTIO_NET_F_CTRL_VQ) == true);

    vi.ctrl.status = cast(ubyte)(~0);
    vi.ctrl.hdr.class__ = dlang_class_alias;
    vi.ctrl.hdr.cmd = cmd;

    sg_init_one(&hdr, &vi.ctrl.hdr, vi.ctrl.hdr.sizeof);
    sgs[out_num] = &hdr;
    out_num++;
    if (dlang_out_alias !is null) {
        sgs[out_num] = dlang_out_alias;
        out_num++;
    }

    sg_init_one(&stat, &vi.ctrl.status, vi.ctrl.status.sizeof);
    sgs[out_num] = &stat;

    assert(out_num + 1 <= ARRAY_SIZE_alex(sgs));
    virtqueue_add_sgs(vi.cvq, sgs.ptr, out_num, 1, vi, GFP_ATOMIC);

    if (!virtqueue_kick(vi.cvq))
        return vi.ctrl.status == VIRTIO_NET_OK;

    while ((virtqueue_get_buf(vi.cvq, &tmp) is null) &&
           !virtqueue_is_broken(vi.cvq))
        cpu_relax();

    return vi.ctrl.status == VIRTIO_NET_OK;
}

extern(C) int virtnet_set_mac_address(net_device *dev, void *p)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    virtio_device *vdev = vi.vdev;
    int ret;
    sockaddr *addr;
    scatterlist sg;

    if (virtio_has_feature(vi.vdev, VIRTIO_NET_F_STANDBY))
        return -EOPNOTSUPP;

    addr = cast(sockaddr *)kmemdup(p, (*addr).sizeof, GFP_KERNEL);
    if (!addr)
        return -ENOMEM;

    ret = eth_prepare_mac_addr_change(dev, addr);
    if (ret)
        goto out_label;

    if (virtio_has_feature(vdev, VIRTIO_NET_F_CTRL_MAC_ADDR)) {
        sg_init_one(&sg, addr.sa_data.ptr, dev.addr_len);
        if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MAC,
                      VIRTIO_NET_CTRL_MAC_ADDR_SET, &sg)) {
            ret = -EINVAL;
            goto out_label;
        }
    } else if (virtio_has_feature(vdev, VIRTIO_NET_F_MAC) &&
           !virtio_has_feature(vdev, VIRTIO_F_VERSION_1)) {
        uint i;

        for (i = 0; i < dev.addr_len; i++)
            virtio_cwrite8(vdev, virtio_net_config.mac.offsetof + i, addr.sa_data[i]);
    }

    eth_commit_mac_addr_change(dev, p);
    ret = 0;

out_label:
    kfree(addr);
    return ret;
}

extern(C) void virtnet_stats(net_device *dev, rtnl_link_stats64 *tot)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    uint start;
    int i;

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);
    assert(vi.max_queue_pairs <= dvi.rq.length);
    assert(vi.max_queue_pairs <= dvi.sq.length);

    for (i = 0; i < vi.max_queue_pairs; i++) {
         ulong tpackets, tbytes, rpackets, rbytes, rdrops;
         receive_queue *rq = &vi.rq[i];
         send_queue *sq = &vi.sq[i];

        do {
            start = u64_stats_fetch_begin(sq.stats.syncp.ptr);
            tpackets = sq.stats.packets;
            tbytes   = sq.stats.bytes;
        } while (u64_stats_fetch_retry_irq(sq.stats.syncp.ptr, start));

        do {
            start = u64_stats_fetch_begin(rq.stats.syncp.ptr);
            rpackets = rq.stats.packets;
            rbytes   = rq.stats.bytes;
            rdrops   = rq.stats.drops;
        } while (u64_stats_fetch_retry_irq(rq.stats.syncp.ptr, start));

        tot.rx_packets += rpackets;
        tot.tx_packets += tpackets;
        tot.rx_bytes   += rbytes;
        tot.tx_bytes   += tbytes;
        tot.rx_dropped += rdrops;
    }

    tot.tx_dropped = dev.stats.tx_dropped;
    tot.tx_fifo_errors = dev.stats.tx_fifo_errors;
    tot.rx_length_errors = dev.stats.rx_length_errors;
    tot.rx_frame_errors = dev.stats.rx_frame_errors;
}

extern(C) void virtnet_ack_link_announce( virtnet_info *vi)
{
    rtnl_lock();
    if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_ANNOUNCE,
                VIRTIO_NET_CTRL_ANNOUNCE_ACK, null))
        __dbind__print_bug();
    rtnl_unlock();
}

extern(C) int _virtnet_set_queues(virtnet_info *vi, ushort queue_pairs)
{
     scatterlist sg;
     net_device *dev = vi.dev;

    if (!vi.has_cvq || !virtio_has_feature(vi.vdev, VIRTIO_NET_F_MQ))
        return 0;

    vi.ctrl.mq.virtqueue_pairs = cpu_to_virtio16(vi.vdev, queue_pairs);
    sg_init_one(&sg, &vi.ctrl.mq, vi.ctrl.mq.sizeof);

    if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MQ,
                  VIRTIO_NET_CTRL_MQ_VQ_PAIRS_SET, &sg)) {
        __dbind__print_bug();
        return -EINVAL;
    } else {
        vi.curr_queue_pairs = queue_pairs;
        if (dev.flags & IFF_UP)
            schedule_delayed_work(&vi.refill, 0);
    }

    return 0;
}

extern(C) int virtnet_set_queues(virtnet_info *vi, ushort queue_pairs)
{
    int err;

    rtnl_lock();
    err = _virtnet_set_queues(vi, queue_pairs);
    rtnl_unlock();
    return err;
}

extern(C) int virtnet_close( net_device *dev)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    int i;

    cancel_delayed_work_sync(&vi.refill);

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);
    assert(vi.max_queue_pairs <= dvi.rq.length);
    assert(vi.max_queue_pairs <= dvi.sq.length);

    for (i = 0; i < vi.max_queue_pairs; i++) {
        xdp_rxq_info_unreg(&vi.rq[i].xdp_rxq);
        napi_disable(&vi.rq[i].napi);
        virtnet_napi_tx_disable(&vi.sq[i].napi);
    }

    return 0;
}

extern(C) void __dbind__netdev_for_each_uc_addr(netdev_hw_addr *ha, net_device *dev, virtio_net_ctrl_mac *mac_data);

extern(C) void __dbind__netdev_for_each_mc_addr(netdev_hw_addr *ha, net_device *dev, virtio_net_ctrl_mac *mac_data);

extern(C) void virtnet_set_rx_mode(net_device *dev)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    scatterlist[2] sg;
    virtio_net_ctrl_mac *mac_data;
    netdev_hw_addr *ha;
    int uc_count;
    int mc_count;
    void *buf;
    int i;

    //[> We can't dynamically set ndo_set_rx_mode, so return gracefully <]
    if (!virtio_has_feature(vi.vdev, VIRTIO_NET_F_CTRL_RX))
        return;

    vi.ctrl.promisc = ((dev.flags & IFF_PROMISC) != 0);
    vi.ctrl.allmulti = ((dev.flags & IFF_ALLMULTI) != 0);

    sg_init_one(&sg[0], &vi.ctrl.promisc, vi.ctrl.promisc.sizeof);

    if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_RX,
                  VIRTIO_NET_CTRL_RX_PROMISC, &sg[0]))
        printk("\x05 aici1");

    sg_init_one(&sg[0], &vi.ctrl.allmulti, vi.ctrl.allmulti.sizeof);

    if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_RX,
                  VIRTIO_NET_CTRL_RX_ALLMULTI, &sg[0]))
        printk("\x05 aici2");

    uc_count = netdev_uc_count(dev);
    mc_count = netdev_mc_count(dev);
    //[> MAC filter - use one buffer for both lists <]
    buf = kzalloc(((uc_count + mc_count) * ETH_ALEN) +
              (2 * mac_data.entries.sizeof), GFP_ATOMIC);
    mac_data = cast(virtio_net_ctrl_mac *)buf;

    if (buf is null)
        return;

    sg_init_table(&sg[0], 2);

    //[> Store the unicast list and count in the front of the buffer <]
    mac_data.entries = cpu_to_virtio32(vi.vdev, uc_count);

    __dbind__netdev_for_each_uc_addr(ha, dev, mac_data);

    sg_set_buf(&sg[0], mac_data,
          cast(uint)(mac_data.entries.sizeof + (uc_count * ETH_ALEN)));

    //[> multicast list and count fill the end <]
    auto m_ptr = mac_data.macs.ptr;
    mac_data = cast(virtio_net_ctrl_mac*)(m_ptr + uc_count * ETH_ALEN);

    mac_data.entries = cpu_to_virtio32(vi.vdev, mc_count);

    __dbind__netdev_for_each_mc_addr(ha, dev, mac_data);

    sg_set_buf(&sg[1], mac_data,
          cast(uint)(mac_data.entries.sizeof + (mc_count * ETH_ALEN)));

    if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MAC,
                  VIRTIO_NET_CTRL_MAC_TABLE_SET, &sg[0]))
        printk("\x05 aici3");

    kfree(buf);
}

extern(C) int virtnet_vlan_rx_add_vid(net_device *dev,
                   ushort proto, ushort vid)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    scatterlist sg;

    vi.ctrl.vid = cpu_to_virtio16(vi.vdev, vid);
    sg_init_one(&sg, &vi.ctrl.vid, vi.ctrl.vid.sizeof);

    if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_VLAN,
                  VIRTIO_NET_CTRL_VLAN_ADD, &sg))
        __dbind__print_bug();
    return 0;
}

extern(C) int virtnet_vlan_rx_kill_vid( net_device *dev,
                    ushort proto, ushort vid)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    scatterlist sg;

    vi.ctrl.vid = cpu_to_virtio16(vi.vdev, vid);
    sg_init_one(&sg, &vi.ctrl.vid, vi.ctrl.vid.sizeof);

    if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_VLAN,
                  VIRTIO_NET_CTRL_VLAN_DEL, &sg))
        __dbind__print_bug();
    return 0;
}

extern(C) void virtnet_clean_affinity(virtnet_info *vi, long hcpu)
{
    int i;

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);

    if (vi.affinity_hint_set) {
        assert(vi.max_queue_pairs <= dvi.rq.length);
        assert(vi.max_queue_pairs <= dvi.sq.length);
        for (i = 0; i < vi.max_queue_pairs; i++) {
            virtqueue_set_affinity(vi.rq[i].vq, null);
            virtqueue_set_affinity(vi.sq[i].vq, null);
        }

        vi.affinity_hint_set = false;
    }
}

extern(C) void virtnet_set_affinity(virtnet_info *vi)
{
    cpumask_var_t mask;
    int stragglers;
    int group_size;
    int i, j, cpu;
    int num_cpu;
    int stride;

    if (!zalloc_cpumask_var(cast(cpumask **) &mask, GFP_KERNEL)) {
        virtnet_clean_affinity(vi, -1);
        return;
    }

    num_cpu = num_online_cpus();
    stride = max(num_cpu / vi.curr_queue_pairs, 1);
    //stride = max_t(int, num_cpu / vi.curr_queue_pairs, 1);
    stragglers = num_cpu >= vi.curr_queue_pairs ?
            num_cpu % vi.curr_queue_pairs :
            0;
    cpu = cpumask_next(-1, cpu_online_mask);

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);
    assert(vi.curr_queue_pairs <= dvi.rq.length);
    assert(vi.curr_queue_pairs <= dvi.sq.length);

    for (i = 0; i < vi.curr_queue_pairs; i++) {
        group_size = stride + (i < stragglers ? 1 : 0);

        for (j = 0; j < group_size; j++) {
            cpumask_set_cpu(cpu, mask.ptr);
            cpu = cpumask_next_wrap(cpu, cpu_online_mask, 8, false);
        }
        virtqueue_set_affinity(vi.rq[i].vq, mask.ptr);
        virtqueue_set_affinity(vi.sq[i].vq, mask.ptr);
        __netif_set_xps_queue(vi.dev, cast(ulong*)cpumask_bits(mask.ptr), cast(ushort)i, false);
        cpumask_clear(mask.ptr);
    }

    vi.affinity_hint_set = true;
    free_cpumask_var(cast(cpumask*)mask);
}

auto hlist_entry_safe_alex(string type, string member)(hlist_node *node) {
    hlist_node * ____ptr = (node);
    if (____ptr !is null)
        return container_of_alex!(type, member)(____ptr);
    else
        return null;
}

extern(C) int virtnet_cpu_online(uint cpu, hlist_node *node)
{
    virtnet_info *vi = hlist_entry_safe_alex!("virtnet_info", "node")(node);
    virtnet_set_affinity(vi);
    return 0;
}

extern(C) int virtnet_cpu_dead(uint cpu, hlist_node *node)
{
    virtnet_info *vi = hlist_entry_safe_alex!("virtnet_info", "node_dead")(node);
    virtnet_set_affinity(vi);
    return 0;
}

extern(C) int virtnet_cpu_down_prep(uint cpu, hlist_node *node)
{
    virtnet_info *vi = hlist_entry_safe_alex!("virtnet_info", "node")(node);

    virtnet_clean_affinity(vi, cpu);
    return 0;
}

extern(C) cpuhp_state __dbind__get_virtionet_online();

extern(C) int virtnet_cpu_notif_add( virtnet_info *vi)
{
    int ret;

    ret = cpuhp_state_add_instance_nocalls(__dbind__get_virtionet_online, &vi.node);
    if (ret)
        return ret;
    ret = cpuhp_state_add_instance_nocalls(cpuhp_state.CPUHP_VIRT_NET_DEAD,
                           &vi.node_dead);
    if (!ret)
        return ret;
    cpuhp_state_remove_instance_nocalls(__dbind__get_virtionet_online, &vi.node);
    return ret;
}

extern(C) void virtnet_cpu_notif_remove( virtnet_info *vi)
{
    cpuhp_state_remove_instance_nocalls(__dbind__get_virtionet_online, &vi.node);
    cpuhp_state_remove_instance_nocalls(cpuhp_state.CPUHP_VIRT_NET_DEAD,
                        &vi.node_dead);
}

extern(C) void virtnet_get_ringparam(net_device *dev,
                 ethtool_ringparam *ring)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);

    ring.rx_max_pending = virtqueue_get_vring_size(dvi.rq[0].vq);
    ring.tx_max_pending = virtqueue_get_vring_size(dvi.sq[0].vq);
    ring.rx_pending = ring.rx_max_pending;
    ring.tx_pending = ring.tx_max_pending;
}

extern(C) void virtnet_get_drvinfo(net_device *dev,
                 ethtool_drvinfo *info)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    virtio_device *vdev = vi.vdev;

    strlcpy(info.driver.ptr, "virtio_net_tmp", info.driver.sizeof);
    strlcpy(info.version_.ptr, VIRTNET_DRIVER_VERSION, info.version_.sizeof);
    strlcpy(info.bus_info.ptr, virtio_bus_name(vdev), info.bus_info.sizeof);

}

extern(C) int virtnet_set_channels( net_device *dev,
                 ethtool_channels *channels)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    ushort queue_pairs = cast(ushort)channels.combined_count;
    int err;

    if (channels.rx_count || channels.tx_count || channels.other_count)
        return -EINVAL;

    if (queue_pairs > vi.max_queue_pairs || queue_pairs == 0)
        return -EINVAL;

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);

    if (dvi.rq[0].xdp_prog)
        return -EINVAL;

    get_online_cpus();
    err = _virtnet_set_queues(vi, queue_pairs);
    if (!err) {
        netif_set_real_num_tx_queues(dev, queue_pairs);
        netif_set_real_num_rx_queues(dev, queue_pairs);

        virtnet_set_affinity(vi);
    }
    put_online_cpus();

    return err;
}

extern(C) void virtnet_get_strings(net_device *dev, uint stringset, ubyte *data)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    char *p = cast(char *)data;
    uint i, j;

    final switch (stringset) {
    case ethtool_stringset.ETH_SS_STATS:
        for (i = 0; i < vi.curr_queue_pairs; i++) {
            for (j = 0; j < VIRTNET_RQ_STATS_LEN; j++) {
                snprintf(p, ETH_GSTRING_LEN, "rx_queue_%u_%s",
                     i, virtnet_rq_stats_desc[j].desc.ptr);
                p += ETH_GSTRING_LEN;
            }
        }

        for (i = 0; i < vi.curr_queue_pairs; i++) {
            for (j = 0; j < VIRTNET_SQ_STATS_LEN; j++) {
                snprintf(p, ETH_GSTRING_LEN, "tx_queue_%u_%s",
                     i, virtnet_sq_stats_desc[j].desc.ptr);
                p += ETH_GSTRING_LEN;
            }
        }
        break;
    }
}

extern(C) int virtnet_get_sset_count(net_device *dev, int sset)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);

    switch (sset) {
    case ethtool_stringset.ETH_SS_STATS:
        return vi.curr_queue_pairs * (VIRTNET_RQ_STATS_LEN +
                           VIRTNET_SQ_STATS_LEN);
    default:
        return -EOPNOTSUPP;
    }
}

extern(C) void virtnet_get_ethtool_stats( net_device *dev,
                       ethtool_stats *stats, ulong *data)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    uint idx = 0, start, i, j;
    const(ubyte) *stats_base;
    size_t offset;

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);
    assert(vi.curr_queue_pairs <= dvi.rq.length);

    for (i = 0; i < vi.curr_queue_pairs; i++) {
        receive_queue *rq = &vi.rq[i];

        stats_base = cast(ubyte *)&rq.stats;
        do {
            start = u64_stats_fetch_begin(rq.stats.syncp.ptr);
            for (j = 0; j < VIRTNET_RQ_STATS_LEN; j++) {
                offset = virtnet_rq_stats_desc[j].offset;
                data[idx + j] = *(cast(ulong *)(stats_base + offset));
            }
        } while (u64_stats_fetch_retry_irq(rq.stats.syncp.ptr, start));
        idx += VIRTNET_RQ_STATS_LEN;
    }

    assert(vi.curr_queue_pairs <= dvi.sq.length);

    for (i = 0; i < vi.curr_queue_pairs; i++) {
        send_queue *sq = &vi.sq[i];

        stats_base = cast(ubyte *)&sq.stats;
        do {
            start = u64_stats_fetch_begin(sq.stats.syncp.ptr);
            for (j = 0; j < VIRTNET_SQ_STATS_LEN; j++) {
                offset = virtnet_sq_stats_desc[j].offset;
                data[idx + j] = *(cast(ulong *)(stats_base + offset));
            }
        } while (u64_stats_fetch_retry_irq(sq.stats.syncp.ptr, start));
        idx += VIRTNET_SQ_STATS_LEN;
    }
}

extern(C) void virtnet_get_channels( net_device *dev,
                  ethtool_channels *channels)
{
     virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);

    channels.combined_count = vi.curr_queue_pairs;
    channels.max_combined = vi.max_queue_pairs;
    channels.max_other = 0;
    channels.rx_count = 0;
    channels.tx_count = 0;
    channels.other_count = 0;
}

extern(C) bool virtnet_validate_ethtool_cmd(const ethtool_link_ksettings *cmd)
{
    ethtool_link_ksettings diff1 = *cmd;
    ethtool_link_ksettings diff2;

    diff1.base.speed = 0;
    diff2.base.port = PORT_OTHER;
    ethtool_link_ksettings_zero_link_mode(&diff1, advertising.ptr);
    diff1.base.duplex = 0;
    diff1.base.cmd = 0;
    diff1.base.link_mode_masks_nwords = 0;

    bool res = !memcmp(&diff1.base, &diff2.base, (diff1.base.sizeof)) &&
    bitmap_empty(diff1.link_modes.supported.ptr, __ETHTOOL_LINK_MODE_MASK_NBITS) &&
    bitmap_empty(diff1.link_modes.advertising.ptr, __ETHTOOL_LINK_MODE_MASK_NBITS) &&
    bitmap_empty(diff1.link_modes.lp_advertising.ptr, __ETHTOOL_LINK_MODE_MASK_NBITS);
    return res;
}

extern(C) int virtnet_set_link_ksettings( net_device *dev,
                      const ethtool_link_ksettings *cmd)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    uint speed;

    speed = cmd.base.speed;

    if (!ethtool_validate_speed(speed) ||
        !ethtool_validate_duplex(cast(ubyte)cmd.base.duplex) ||
        !virtnet_validate_ethtool_cmd(cmd))
        return -EINVAL;
    vi.speed = speed;
    vi.duplex = cast(ubyte)cmd.base.duplex;

    return 0;
}

extern(C) int virtnet_get_link_ksettings( net_device *dev,
                       ethtool_link_ksettings *cmd)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);

    cmd.base.speed = vi.speed;
    cmd.base.duplex = vi.duplex;
    cmd.base.port = PORT_OTHER;

    return 0;
}

extern(C) void virtnet_init_settings( net_device *dev)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);

    vi.speed = SPEED_UNKNOWN;
    vi.duplex = DUPLEX_UNKNOWN;
}

extern(C) void virtnet_update_settings(virtnet_info *vi)
{
    uint speed;
    ubyte duplex;

    if (!virtio_has_feature(vi.vdev, VIRTIO_NET_F_SPEED_DUPLEX))
        return;

    speed = virtio_cread32(vi.vdev, virtio_net_config.speed.offsetof);
    if (ethtool_validate_speed(speed))
        vi.speed = speed;
    duplex = cast(ubyte)virtio_cread8(vi.vdev, virtio_net_config.duplex.offsetof);
    if (ethtool_validate_duplex(duplex))
        vi.duplex = duplex;
}

extern(C) void virtnet_freeze_down( virtio_device *vdev)
{
    virtnet_info *vi = cast(virtnet_info *) vdev.priv;
    int i;

    /* Make sure no work handler is accessing the device */
    flush_work(&vi.config_work);

    netif_tx_lock_bh(vi.dev);
    netif_device_detach(vi.dev);
    netif_tx_unlock_bh(vi.dev);
    cancel_delayed_work_sync(&vi.refill);

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);

    if (netif_running(vi.dev)) {
        assert(vi.max_queue_pairs <= dvi.rq.length);
        assert(vi.max_queue_pairs <= dvi.sq.length);
        for (i = 0; i < vi.max_queue_pairs; i++) {
            napi_disable(&vi.rq[i].napi);
            virtnet_napi_tx_disable(&vi.sq[i].napi);
        }
    }
}

extern(C) int virtnet_restore_up(virtio_device *vdev)
{
    virtnet_info *vi = cast(virtnet_info *) vdev.priv;
    int err, i;

    err = init_vqs(vi);
    if (err)
        return err;

    virtio_device_ready(vdev);

    if (netif_running(vi.dev)) {

        dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
        assert(dvi.vi == vi);
        assert(vi.curr_queue_pairs <= dvi.rq.length);
        assert(vi.curr_queue_pairs <= dvi.sq.length);

        for (i = 0; i < vi.curr_queue_pairs; i++)
            if (!try_fill_recv(vi, &vi.rq[i], GFP_KERNEL))
                schedule_delayed_work(&vi.refill, 0);

        for (i = 0; i < vi.max_queue_pairs; i++) {
            virtnet_napi_enable(vi.rq[i].vq, &vi.rq[i].napi);
            virtnet_napi_tx_enable(vi, vi.sq[i].vq,
                           &vi.sq[i].napi);
        }
    }

    netif_tx_lock_bh(vi.dev);
    netif_device_attach(vi.dev);
    netif_tx_unlock_bh(vi.dev);
    return err;
}

extern(C) int virtnet_set_guest_offloads(virtnet_info *vi, ulong offloads)
{
    scatterlist sg;
    vi.ctrl.offloads = cpu_to_virtio64(vi.vdev, offloads);

    sg_init_one(&sg, &vi.ctrl.offloads, vi.ctrl.offloads.sizeof);

    if (!virtnet_send_command(vi, VIRTIO_NET_CTRL_GUEST_OFFLOADS,
                  VIRTIO_NET_CTRL_GUEST_OFFLOADS_SET, &sg)) {
        __dbind__print_bug();
        return -EINVAL;
    }

    return 0;
}

extern(C) int virtnet_clear_guest_offloads( virtnet_info *vi)
{
    ulong offloads = 0;

    if (!vi.guest_offloads)
        return 0;

    if (virtio_has_feature(vi.vdev, VIRTIO_NET_F_GUEST_CSUM))
        offloads = 1UL << VIRTIO_NET_F_GUEST_CSUM;

    return virtnet_set_guest_offloads(vi, offloads);
}

extern(C) int virtnet_restore_guest_offloads( virtnet_info *vi)
{
    ulong offloads = vi.guest_offloads;

    if (!vi.guest_offloads)
        return 0;
    if (virtio_has_feature(vi.vdev, VIRTIO_NET_F_GUEST_CSUM))
        offloads |= 1UL << VIRTIO_NET_F_GUEST_CSUM;

    return virtnet_set_guest_offloads(vi, offloads);
}

extern(C) void __dbind__rcu_assign_pointer(bpf_prog *p, bpf_prog* v);

extern(C) int virtnet_xdp_set(net_device *dev, bpf_prog *prog, netlink_ext_ack *extack)
{
    c_ulong max_sz = PAGE_SIZE - padded_vnet_hdr.sizeof;
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    bpf_prog *old_prog;
    ushort xdp_qp = 0, curr_qp;
    int i, err;

    if (!virtio_has_feature(vi.vdev, VIRTIO_NET_F_CTRL_GUEST_OFFLOADS)
        && (virtio_has_feature(vi.vdev, VIRTIO_NET_F_GUEST_TSO4) ||
            virtio_has_feature(vi.vdev, VIRTIO_NET_F_GUEST_TSO6) ||
            virtio_has_feature(vi.vdev, VIRTIO_NET_F_GUEST_ECN) ||
        virtio_has_feature(vi.vdev, VIRTIO_NET_F_GUEST_UFO))) {
        __dbind__print_bug();
        return -EOPNOTSUPP;
    }

    if (vi.mergeable_rx_bufs && !vi.any_header_sg) {
        __dbind__print_bug();
        return -EINVAL;
    }

    if (dev.mtu > max_sz) {
        __dbind__print_bug();
        return -EINVAL;
    }

    curr_qp = cast(ushort)(vi.curr_queue_pairs - vi.xdp_queue_pairs);
    if (prog !is null)
        xdp_qp = cast(ushort) nr_cpu_ids;

    if (curr_qp + xdp_qp > vi.max_queue_pairs) {
        __dbind__print_bug();
        return -ENOMEM;
    }

    if (prog !is null) {
        prog = bpf_prog_add(prog, vi.max_queue_pairs - 1);
        if (IS_ERR(prog))
            return cast(int)PTR_ERR(prog);
    }

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);
    assert(vi.max_queue_pairs <= dvi.rq.length);
    assert(vi.max_queue_pairs <= dvi.sq.length);

    if (netif_running(dev))
        for (i = 0; i < vi.max_queue_pairs; i++)
            napi_disable(&vi.rq[i].napi);

    netif_set_real_num_rx_queues(dev, curr_qp + xdp_qp);
    err = _virtnet_set_queues(vi, cast(ushort)(curr_qp + xdp_qp));
    if (err)
        goto err;
    vi.xdp_queue_pairs = xdp_qp;

    for (i = 0; i < vi.max_queue_pairs; i++) {
        old_prog = vi.rq[i].xdp_prog;
        __dbind__rcu_assign_pointer(vi.rq[i].xdp_prog, prog);
        if (i == 0) {
            if (old_prog is null)
                virtnet_clear_guest_offloads(vi);
            if (prog is null)
                virtnet_restore_guest_offloads(vi);
        }
        if (old_prog !is null)
            bpf_prog_put(old_prog);
        if (netif_running(dev))
            virtnet_napi_enable(vi.rq[i].vq, &vi.rq[i].napi);
    }

    return 0;

err:
    for (i = 0; i < vi.max_queue_pairs; i++)
        virtnet_napi_enable(vi.rq[i].vq, &vi.rq[i].napi);
    if (prog)
        bpf_prog_sub(prog, vi.max_queue_pairs - 1);
    return err;
}

extern(C) uint virtnet_xdp_query(net_device *dev)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    const(bpf_prog) *xdp_prog;
    int i;

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);
    assert(vi.max_queue_pairs <= dvi.rq.length);

    for (i = 0; i < vi.max_queue_pairs; i++) {
        xdp_prog = vi.rq[i].xdp_prog;
        if (xdp_prog !is null)
            return xdp_prog.aux.id;
    }
    return 0;
}

extern(C) int virtnet_xdp(net_device *dev, netdev_bpf *xdp)
{
    switch (xdp.command) {
    case bpf_netdev_command.XDP_SETUP_PROG:
        return virtnet_xdp_set(dev, xdp.prog, xdp.extack);
    case bpf_netdev_command.XDP_QUERY_PROG:
        xdp.prog_id = virtnet_xdp_query(dev);
        return 0;
    default:
        return -EINVAL;
    }
}

extern(C) int virtnet_get_phys_port_name( net_device *dev, char *buf,
                      size_t len)
{
    virtnet_info *vi = cast(virtnet_info *) netdev_priv(dev);
    int ret;

    if (!virtio_has_feature(vi.vdev, VIRTIO_NET_F_STANDBY))
        return -EOPNOTSUPP;

    ret = snprintf(buf, len, "sby");
    if (ret >= len)
        return -EOPNOTSUPP;

    return 0;
}

extern(C) int __dbind__virtio_cread_feature_1(virtio_device *vdev, int fbit, ushort *ptr);

extern(C) void virtnet_config_changed_work(work_struct *work)
{
    virtnet_info *vi = container_of_alex!("virtnet_info", "config_work")(work);
    ushort v;

    if (__dbind__virtio_cread_feature_1(vi.vdev, VIRTIO_NET_F_STATUS, &v) < 0)
        return;

    if (v & VIRTIO_NET_S_ANNOUNCE) {
        netdev_notify_peers(vi.dev);
        virtnet_ack_link_announce(vi);
    }

    v &= VIRTIO_NET_S_LINK_UP;

    if (vi.status == v)
        return;

    vi.status = v;

    if (vi.status & VIRTIO_NET_S_LINK_UP) {
        virtnet_update_settings(vi);
        netif_carrier_on(vi.dev);
        netif_tx_wake_all_queues(vi.dev);
    } else {
        netif_carrier_off(vi.dev);
        netif_tx_stop_all_queues(vi.dev);
    }
}

extern(C) void virtnet_config_changed( virtio_device *vdev)
{
    virtnet_info *vi = cast(virtnet_info *) vdev.priv;

    schedule_work(&vi.config_work);
}

extern(C) void virtnet_free_queues(virtnet_info *vi)
{
    int i;

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);
    assert(vi.max_queue_pairs <= dvi.rq.length);
    assert(vi.max_queue_pairs <= dvi.sq.length);

    for (i = 0; i < vi.max_queue_pairs; i++) {
        napi_hash_del(&vi.rq[i].napi);
        netif_napi_del(&vi.rq[i].napi);
        netif_napi_del(&vi.sq[i].napi);
    }

    synchronize_net();

   kfree(dvi);
   kfree(vi.ctrl);
}

extern(C) void __dbind__RCU_INIT_POINTER_null(bpf_prog *xdp);

extern(C) void _free_receive_bufs(virtnet_info *vi)
{
    bpf_prog *old_prog;
    int i;

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);
    assert(vi.max_queue_pairs <= dvi.rq.length);

    for (i = 0; i < vi.max_queue_pairs; i++) {
        while (vi.rq[i].pages)
            __free_pages(get_a_page(&vi.rq[i], GFP_KERNEL), 0);

        old_prog = vi.rq[i].xdp_prog;
        __dbind__RCU_INIT_POINTER_null(vi.rq[i].xdp_prog);
        if (old_prog !is null)
            bpf_prog_put(old_prog);
    }
}

extern(C) void free_receive_bufs( virtnet_info *vi)
{
    rtnl_lock();
    _free_receive_bufs(vi);
    rtnl_unlock();
}

extern(C) void free_receive_page_frags( virtnet_info *vi)
{
    int i;
    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);
    assert(vi.max_queue_pairs <= dvi.rq.length);

    for (i = 0; i < vi.max_queue_pairs; i++)
        if (vi.rq[i].alloc_frag.page_)
            put_page(vi.rq[i].alloc_frag.page_);
}

extern(C) bool is_xdp_raw_buffer_queue( virtnet_info *vi, int q)
{
    if (q < (vi.curr_queue_pairs - vi.xdp_queue_pairs))
        return false;
    else if (q < vi.curr_queue_pairs)
        return true;
    else
        return false;
}

extern(C) void free_unused_bufs(virtnet_info *vi)
{
    void *buf;
    int i;

    dlang_virtnet_info *dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);
    assert(vi.max_queue_pairs <= dvi.rq.length);
    assert(vi.max_queue_pairs <= dvi.sq.length);

    for (i = 0; i < vi.max_queue_pairs; i++) {
        virtqueue *vq = vi.sq[i].vq;
        while ((buf = virtqueue_detach_unused_buf(vq)) != null) {
            if (!is_xdp_raw_buffer_queue(vi, i))
                dev_kfree_skb(cast(sk_buff*)buf);
            else
                put_page(virt_to_head_page(buf));
        }
    }

    for (i = 0; i < vi.max_queue_pairs; i++) {
         virtqueue *vq = vi.rq[i].vq;

        while ((buf = virtqueue_detach_unused_buf(vq)) != null) {
            if (vi.mergeable_rx_bufs) {
                put_page(virt_to_head_page(buf));
            } else if (vi.big_packets) {
                give_pages(&vi.rq[i], cast(page*)buf);
            } else {
                put_page(virt_to_head_page(buf));
            }
        }
    }
}

extern(C) void virtnet_del_vqs( virtnet_info *vi)
{
    virtio_device *vdev = vi.vdev;

    virtnet_clean_affinity(vi, -1);

    vdev.config.del_vqs(vdev);

    virtnet_free_queues(vi);
}

 //How large should a single buffer be so a queue full of these can fit at
 //least one full packet?
 //Logic below assumes the mergeable buffer header is used.
enum IP_MAX_MTU = 0xFFFFU;

extern(C) uint mergeable_min_buf_len( virtnet_info *vi,  virtqueue *vq)
{
    const uint hdr_len = virtio_net_hdr_mrg_rxbuf.sizeof;
    uint rq_size = virtqueue_get_vring_size(vq);
    uint packet_len = vi.big_packets ? IP_MAX_MTU : vi.dev.max_mtu;
    uint buf_len = hdr_len + ETH_HLEN + VLAN_HLEN + packet_len;
    uint min_buf_len = (buf_len + rq_size - 1) / rq_size;

    return max(max(min_buf_len, hdr_len) - hdr_len, cast(uint)GOOD_PACKET_LEN);
}

extern(C) void *addr_skb_recv_done();
extern(C) void *addr_skb_xmit_done();

extern(C) int virtnet_find_vqs(virtnet_info *vi)
{
    vq_callback_t *callbacks;
    virtqueue **vqs;
    dlang_virtnet_info *dvi;
    int ret = -ENOMEM;
    int i, total_vqs;
    const(char) **names;
    bool *ctx;

     /*[>We expect 1 RX virtqueue followed by 1 TX virtqueue, followed by<]*/
     /*[>possible N-1 RX/TX queue pairs used in multiqueue mode, followed by<]*/
     /*[>possible control vq.<]*/
    total_vqs = vi.max_queue_pairs * 2 +
            virtio_has_feature(vi.vdev, VIRTIO_NET_F_CTRL_VQ);

    /*[>[> Allocate space for find_vqs parameters <]<]*/
    vqs = cast(virtqueue**)kcalloc(total_vqs, (*vqs).sizeof, GFP_KERNEL);
    if (!vqs)
        goto err_vq;
    //grija!!!
    callbacks = cast(vq_callback_t*)kmalloc_array(total_vqs, (*callbacks).sizeof, GFP_KERNEL);
    if (!callbacks)
        goto err_callback;
    //grija!!!
    names = cast(const(char)**)kmalloc_array(total_vqs, (*names).sizeof, GFP_KERNEL);
    if (!names)
        goto err_names;
    if (!vi.big_packets || vi.mergeable_rx_bufs) {
        ctx = cast(bool*)kcalloc(total_vqs, (*ctx).sizeof, GFP_KERNEL);
        if (!ctx)
            goto err_ctx;
    } else {
        ctx = null;
    }

    /*[>[> Parameters for control virtqueue, if any <]<]*/
    if (vi.has_cvq) {
        callbacks[total_vqs - 1] = null;
        names[total_vqs - 1] = "control";
    }

    /*[>[> Allocate/initialize parameters for send/receive virtqueues <]<]*/
    dvi = container_of_alex!("dlang_virtnet_info", "tmp")(vi.sq);
    assert(dvi.vi == vi);
    assert(vi.max_queue_pairs <= dvi.rq.length);
    assert(vi.max_queue_pairs <= dvi.sq.length);

    for (i = 0; i < vi.max_queue_pairs; i++) {
        callbacks[rxq2vq(i)] = cast(vq_callback_t)addr_skb_recv_done;
        callbacks[txq2vq(i)] = cast(vq_callback_t)addr_skb_xmit_done;
        sprintf(vi.rq[i].name.ptr, "input.%d", i);
        sprintf(vi.sq[i].name.ptr, "output.%d", i);
        names[rxq2vq(i)] = vi.rq[i].name.ptr;
        names[txq2vq(i)] = vi.sq[i].name.ptr;
        if (ctx)
            ctx[rxq2vq(i)] = true;
    }

    ret = vi.vdev.config.find_vqs(vi.vdev, total_vqs, vqs, callbacks,
                     names, ctx, null);
    if (ret)
        goto err_find;

    if (vi.has_cvq) {
        vi.cvq = vqs[total_vqs - 1];
        if (virtio_has_feature(vi.vdev, VIRTIO_NET_F_CTRL_VLAN))
            vi.dev.features |= __NETIF_F(HW_VLAN_CTAG_FILTER);
    }

    for (i = 0; i < vi.max_queue_pairs; i++) {
        vi.rq[i].vq = vqs[rxq2vq(i)];
        vi.rq[i].min_buf_len = mergeable_min_buf_len(vi, vi.rq[i].vq);
        vi.sq[i].vq = vqs[txq2vq(i)];
    }

    /*[>[> run here: ret == 0. <]<]*/

err_find:
    kfree(ctx);
err_ctx:
    kfree(names);
err_names:
    kfree(callbacks);
err_callback:
    kfree(vqs);
err_vq:
    return ret;
}

extern(C) void __dbind__netif_napi_add(net_device *dev, napi_struct *napi,
             int weight);
extern(C) void __dbind__netif_tx_napi_add(net_device *dev,
                     napi_struct *napi,
                     int weight);

extern(C) void __dbind__INIT_DELAYED_WORK(delayed_work* dw);
extern(C) void __dbind__ewma_pkt_len_init(ewma_pkt_len *e);

extern(C) int virtnet_alloc_queues(virtnet_info *vi)
{
    int i;
    size_t sq_size;
    size_t rq_size;
    size_t dvi_total_size;
    dlang_virtnet_info* dvi;

    vi.ctrl = cast(control_buf*)kzalloc((*vi.ctrl).sizeof, GFP_KERNEL);
    if (vi.ctrl is null)
        goto err_ctrl;

    sq_size = (*vi.sq).sizeof * vi.max_queue_pairs;
    rq_size = (*vi.rq).sizeof * vi.max_queue_pairs;
    dvi_total_size = dlang_virtnet_info.sizeof + sq_size + rq_size;
    dvi = cast(dlang_virtnet_info*) kzalloc(dvi_total_size, GFP_KERNEL);
    dvi.sq = (cast(send_queue*)(dvi.tmp.ptr))[0 .. vi.max_queue_pairs];

    if (dvi.sq is null)
        goto err_sq;

    vi.sq = dvi.sq.ptr;
    dvi.vi = vi;

    dvi.rq = (cast(receive_queue*)(dvi.tmp.ptr + sq_size))[0 .. vi.max_queue_pairs];

    if (dvi.rq is null)
        goto err_rq;
    vi.rq = dvi.rq.ptr;

    __dbind__INIT_DELAYED_WORK(&vi.refill);
    for (i = 0; i < vi.max_queue_pairs; i++) {
        vi.rq[i].pages = null;
        __dbind__netif_napi_add(vi.dev, &vi.rq[i].napi, napi_weight);
        __dbind__netif_tx_napi_add(vi.dev, &vi.sq[i].napi, napi_tx ? napi_weight : 0);

        sg_init_table(vi.rq[i].sg.ptr, cast(uint)ARRAY_SIZE_alex(vi.rq[i].sg));
        __dbind__ewma_pkt_len_init(&vi.rq[i].mrg_avg_pkt_len);
        sg_init_table(vi.sq[i].sg.ptr, cast(uint)ARRAY_SIZE_alex(vi.sq[i].sg));

        u64_stats_init(vi.rq[i].stats.syncp.ptr);
        u64_stats_init(vi.sq[i].stats.syncp.ptr);
    }

    return 0;

err_rq:
    kfree(vi.sq);
err_sq:
    kfree(vi.ctrl);
err_ctrl:
    return -ENOMEM;
}

extern(C) int init_vqs( virtnet_info *vi)
{
    int ret;

    //[> Allocate send & receive queues <]
    ret = virtnet_alloc_queues(vi);
    if (ret)
        goto err;

    ret = virtnet_find_vqs(vi);
    if (ret)
        goto err_free;

    get_online_cpus();
    virtnet_set_affinity(vi);
    put_online_cpus();

    return 0;

err_free:
    virtnet_free_queues(vi);
err:
    return ret;
}

extern(C) bool virtnet_validate_features(virtio_device *vdev)
{
    if (!virtio_has_feature(vdev, VIRTIO_NET_F_CTRL_VQ) &&
        (virtnet_fail_on_feature(vdev, VIRTIO_NET_F_CTRL_RX,
               "VIRTIO_NET_F_CTRL_RX", "VIRTIO_NET_F_CTRL_VQ") ||
         virtnet_fail_on_feature(vdev, VIRTIO_NET_F_CTRL_VLAN,
             "VIRTIO_NET_F_CTRL_VLAN", "VIRTIO_NET_F_CTRL_VQ") ||
         virtnet_fail_on_feature(vdev, VIRTIO_NET_F_GUEST_ANNOUNCE,
               "VIRTIO_NET_F_GUEST_ANNOUNCE", "VIRTIO_NET_F_CTRL_VQ") ||
         virtnet_fail_on_feature(vdev, VIRTIO_NET_F_MQ, "VIRTIO_NET_F_MQ",
             "VIRTIO_NET_F_CTRL_VQ") ||
         virtnet_fail_on_feature(vdev, VIRTIO_NET_F_CTRL_MAC_ADDR,
             "VIRTIO_NET_F_CTRL_MAC_ADDR", "VIRTIO_NET_F_CTRL_VQ"))) {
        return false;
    }

    return true;
}

extern(C) int virtnet_validate(virtio_device *vdev)
{
    if (vdev.config.get is null) {
        printk("%s failure: config access disabled\n");
        return -EINVAL;
    }

    if (!virtnet_validate_features(vdev))
        return -EINVAL;

    if (virtio_has_feature(vdev, VIRTIO_NET_F_MTU)) {
        int mtu = virtio_cread16(vdev, virtio_net_config.mtu.offsetof);
        if (mtu < ETH_MIN_MTU)
            __virtio_clear_bit(vdev, VIRTIO_NET_F_MTU);
    }

    return 0;
}

extern(C) int __dbind__virtio_cread_feature_2(virtio_device *, int, ushort *);
extern(C) void* __dbind__get_virtnet_netdev_addr();
extern(C) void* __dbind__get_ethtool_ops_addr();
extern(C) dstruct_failover *__dbind__net_failover_create(net_device *standby_dev);
extern(C) void net_failover_destroy(dstruct_failover *);
extern(C) void __dbind__INIT_WORK(work_struct *ws);
extern(C) attribute_group * __dbind__get_mrg_rx_group();

extern(C) int virtnet_probe(virtio_device *vdev)
{
    int i, err = -ENOMEM;
    net_device *dev;
    virtnet_info *vi;
    ushort max_queue_pairs;
    int mtu;

    //[> Find if host supports multiqueue virtio_net device <]
    err = __dbind__virtio_cread_feature_2(vdev, VIRTIO_NET_F_MQ, &max_queue_pairs);

    //[> We need at least 2 queue's <]
    if (err || max_queue_pairs < VIRTIO_NET_CTRL_MQ_VQ_PAIRS_MIN ||
        max_queue_pairs > VIRTIO_NET_CTRL_MQ_VQ_PAIRS_MAX ||
        !virtio_has_feature(vdev, VIRTIO_NET_F_CTRL_VQ))
        max_queue_pairs = 1;

    //[> Allocate ourselves a network device with room for our info <]
    dev = alloc_etherdev_mq(virtnet_info.sizeof, max_queue_pairs);
    if (dev is null)
        return -ENOMEM;

    //[> Set up network device as normal. <]
    dev.priv_flags |= netdev_priv_flags.IFF_UNICAST_FLT | netdev_priv_flags.IFF_LIVE_ADDR_CHANGE;
    //dev.netdev_ops = &virtnet_netdev;
    dev.netdev_ops = cast(net_device_ops*)__dbind__get_virtnet_netdev_addr();
    dev.features = __NETIF_F(HIGHDMA);

    dev.ethtool_ops_ = cast(ethtool_ops*)__dbind__get_ethtool_ops_addr();
    dev.dev.parent = &vdev.dev;

    //[> Do we support "hardware" checksums? <]
    if (virtio_has_feature(vdev, VIRTIO_NET_F_CSUM)) {
        //[> This opens up the world of extra features. <]
        dev.hw_features |= __NETIF_F(HW_CSUM) | __NETIF_F(SG);
        if (csum)
            dev.features |= __NETIF_F(HW_CSUM) | __NETIF_F(SG);

        if (virtio_has_feature(vdev, VIRTIO_NET_F_GSO)) {
            dev.hw_features |= __NETIF_F(TSO)
                | __NETIF_F(TSO_ECN) | __NETIF_F(TSO6);
        }
        //[> Individual feature bits: what can host handle? <]
        if (virtio_has_feature(vdev, VIRTIO_NET_F_HOST_TSO4))
            dev.hw_features |= __NETIF_F(TSO);
        if (virtio_has_feature(vdev, VIRTIO_NET_F_HOST_TSO6))
            dev.hw_features |= __NETIF_F(TSO6);
        if (virtio_has_feature(vdev, VIRTIO_NET_F_HOST_ECN))
            dev.hw_features |= __NETIF_F(TSO_ECN);

        dev.features |= __NETIF_F(GSO_ROBUST);

        if (gso)
            dev.features |= dev.hw_features & NETIF_F_ALL_TSO;
        //[> (!csum && gso) case will be fixed by register_netdev() <]
    }
    if (virtio_has_feature(vdev, VIRTIO_NET_F_GUEST_CSUM))
        dev.features |= __NETIF_F(RXCSUM);

    dev.vlan_features = dev.features;

    //[> MTU range: 68 - 65535 <]
    dev.min_mtu = ETH_MIN_MTU;
    dev.max_mtu = ETH_MAX_MTU;

    //[> Configuration may specify what MAC to use.  Otherwise random. <]
    if (virtio_has_feature(vdev, VIRTIO_NET_F_MAC))
        virtio_cread_bytes(vdev,
                	   virtio_net_config.mac.offsetof,
                	   dev.dev_addr, dev.addr_len);
    else
        eth_hw_addr_random(dev);

    //[> Set up our device-specific information <]
    vi = cast(virtnet_info *) netdev_priv(dev);
    vi.dev = dev;
    vi.vdev = vdev;
    vdev.priv = vi;

    __dbind__INIT_WORK(&vi.config_work);

    //[> If we can receive ANY GSO packets, we must allocate large ones. <]
    if (virtio_has_feature(vdev, VIRTIO_NET_F_GUEST_TSO4) ||
        virtio_has_feature(vdev, VIRTIO_NET_F_GUEST_TSO6) ||
        virtio_has_feature(vdev, VIRTIO_NET_F_GUEST_ECN) ||
        virtio_has_feature(vdev, VIRTIO_NET_F_GUEST_UFO))
        vi.big_packets = true;

    if (virtio_has_feature(vdev, VIRTIO_NET_F_MRG_RXBUF))
        vi.mergeable_rx_bufs = true;

    if (virtio_has_feature(vdev, VIRTIO_NET_F_MRG_RXBUF) ||
        virtio_has_feature(vdev, VIRTIO_F_VERSION_1))
        vi.hdr_len = virtio_net_hdr_mrg_rxbuf.sizeof;
    else
        vi.hdr_len = virtio_net_hdr.sizeof;

    if (virtio_has_feature(vdev, VIRTIO_F_ANY_LAYOUT) ||
        virtio_has_feature(vdev, VIRTIO_F_VERSION_1))
        vi.any_header_sg = true;

    if (virtio_has_feature(vdev, VIRTIO_NET_F_CTRL_VQ))
        vi.has_cvq = true;

    if (virtio_has_feature(vdev, VIRTIO_NET_F_MTU)) {
        mtu = virtio_cread16(vdev, virtio_net_config.mtu.offsetof);
        if (mtu < dev.min_mtu) {
            //Should never trigger: MTU was previously validated
             //in virtnet_validate.
            printk("device MTU appears to have changed\n");
            goto free;
        }

        dev.mtu = mtu;
        dev.max_mtu = mtu;

        //[> TODO: size buffers correctly in this case. <]
        if (dev.mtu > ETH_DATA_LEN)
            vi.big_packets = true;
    }

    if (vi.any_header_sg)
        dev.needed_headroom = vi.hdr_len;

    //[> Enable multiqueue by default <]
    if (num_online_cpus() >= max_queue_pairs)
        vi.curr_queue_pairs = cast(ushort)max_queue_pairs;
    else
        vi.curr_queue_pairs = cast(ushort)num_online_cpus();
    vi.max_queue_pairs = cast(ushort)max_queue_pairs;

    //[> Allocate/initialize the rx/tx queues, and invoke find_vqs <]
    err = init_vqs(vi);
    if (err)
        goto free;

    version(CONFIG_SYSFS) {
        if (vi.mergeable_rx_bufs)
            dev.sysfs_rx_queue_group = __dbind__get_mrg_rx_group();
    }
    netif_set_real_num_tx_queues(dev, vi.curr_queue_pairs);
    netif_set_real_num_rx_queues(dev, vi.curr_queue_pairs);

    virtnet_init_settings(dev);

    if (virtio_has_feature(vdev, VIRTIO_NET_F_STANDBY)) {
        vi.failover = __dbind__net_failover_create(vi.dev);
        if (IS_ERR(vi.failover)) {
            err = cast(int)PTR_ERR(vi.failover);
            goto free_vqs;
        }
    }

    err = register_netdev(dev);
    if (err) {
        printk("virtio_net: registering device failed\n");
        goto free_failover;
    }

    virtio_device_ready(vdev);

    err = virtnet_cpu_notif_add(vi);
    if (err) {
        printk("virtio_net: registering cpu notifier failed\n");
        goto free_unregister_netdev;
    }

    virtnet_set_queues(vi, vi.curr_queue_pairs);

    //[> Assume link up if device can't report link status,
       //otherwise get link status from config. */
    netif_carrier_off(dev);
    if (virtio_has_feature(vi.vdev, VIRTIO_NET_F_STATUS)) {
        schedule_work(&vi.config_work);
    } else {
        vi.status = VIRTIO_NET_S_LINK_UP;
        virtnet_update_settings(vi);
        netif_carrier_on(dev);
    }

    //for (i = 0; i < ARRAY_SIZE_alex(guest_offloads); i++)
    for (i = 0; i < 4; i++)
        if (virtio_has_feature(vi.vdev, cast(uint)guest_offloads[i]))
            set_bit(cast(int)guest_offloads[i], &vi.guest_offloads);

    printk("virtnet: registered device %s with %d RX and TX vq's\n",
         dev.name.ptr, max_queue_pairs);

    return 0;

free_unregister_netdev:
    vi.vdev.config.reset(vdev);

    unregister_netdev(dev);
free_failover:
    net_failover_destroy(vi.failover);
free_vqs:
    cancel_delayed_work_sync(&vi.refill);
    free_receive_page_frags(vi);
    virtnet_del_vqs(vi);
free:
    free_netdev(dev);
    return err;
}

extern(C) void remove_vq_common( virtnet_info *vi)
{
    vi.vdev.config.reset(vi.vdev);

    /* Free unused buffers in both send and recv, if any. */
    free_unused_bufs(vi);

    free_receive_bufs(vi);

    free_receive_page_frags(vi);

    virtnet_del_vqs(vi);
}

extern(C) void virtnet_remove( virtio_device *vdev)
{
    virtnet_info *vi = cast(virtnet_info *) vdev.priv;

    virtnet_cpu_notif_remove(vi);

    /* Make sure no work handler is accessing the device. */
    flush_work(&vi.config_work);

    unregister_netdev(vi.dev);

    net_failover_destroy(vi.failover);

    remove_vq_common(vi);

    free_netdev(vi.dev);
}

extern(C) int virtnet_freeze( virtio_device *vdev)
{
    virtnet_info *vi = cast(virtnet_info *) vdev.priv;

    virtnet_cpu_notif_remove(vi);
    virtnet_freeze_down(vdev);
    remove_vq_common(vi);

    return 0;
}

extern(C) int virtnet_restore(virtio_device *vdev)
{
    virtnet_info *vi = cast(virtnet_info *) vdev.priv;
    int err;

    err = virtnet_restore_up(vdev);
    if (err)
        return err;
    virtnet_set_queues(vi, vi.curr_queue_pairs);

    err = virtnet_cpu_notif_add(vi);
    if (err)
        return err;

    return 0;
}
